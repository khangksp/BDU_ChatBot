#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Comprehensive ChatBot Testing Script for BDU Lecturer Assistant
M·ª•c ƒë√≠ch: Test t·∫•t c·∫£ c√¢u h·ªèi v√† t√¨m ra nh·ªØng keyword/pattern c√≤n thi·∫øu
"""

import os
import sys
import django
import pandas as pd
import json
import time
from datetime import datetime
from collections import defaultdict, Counter
import re

# Setup Django environment
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'backend.settings')  # Fixed: backend.settings
django.setup()

# Import your chatbot services
from ai_models.services import HybridChatbotAI  # Fixed: ai_models instead of ai_services

class ChatBotTester:
    """Comprehensive Testing Suite for ChatBot"""
    
    def __init__(self):
        self.chatbot = HybridChatbotAI()
        self.test_results = []
        self.failed_queries = []
        self.keyword_analysis = defaultdict(list)
        self.session_id = f"test_session_{int(time.time())}"
        
        # ‚úÖ CATEGORIES ƒë·ªÉ ph√¢n lo·∫°i c√¢u h·ªèi
        self.categories = {
            'tuition_fees': ['h·ªçc ph√≠', 'l·ªá ph√≠', 'ph√≠', 'ti·ªÅn', 'chi ph√≠', 'thanh to√°n'],
            'admission': ['tuy·ªÉn sinh', 'nh·∫≠p h·ªçc', 'ƒëƒÉng k√Ω', 'x√©t tuy·ªÉn', 'ƒëi·ªÅu ki·ªán'],
            'graduation': ['t·ªët nghi·ªáp', 'b·∫±ng', 'vƒÉn b·∫±ng', 'nh·∫≠n b·∫±ng', 't·ªët nghi·ªáp'],
            'lecturer_tasks': ['ng√¢n h√†ng ƒë·ªÅ thi', 'k√™ khai', 'nhi·ªám v·ª•', 'b√°o c√°o', 'gi·∫£ng vi√™n'],
            'academic_journal': ['t·∫°p ch√≠', 'b√†i vi·∫øt', 'nghi√™n c·ª©u', 'khoa h·ªçc'],
            'competition_awards': ['thi ƒëua', 'khen th∆∞·ªüng', 'danh hi·ªáu', 'b·∫±ng khen'],
            'schedule': ['l·ªãch', 'th·ªùi kh√≥a bi·ªÉu', 'gi·∫£ng d·∫°y', 'h·ªçc'],
            'departments': ['ph√≤ng', 'khoa', 'b·ªô m√¥n', 'ƒë∆°n v·ªã'],
            'facilities': ['c∆° s·ªü', 'ph√≤ng h·ªçc', 'th∆∞ vi·ªán', 'lab'],
            'programs': ['ng√†nh', 'chuy√™n ng√†nh', 'ch∆∞∆°ng tr√¨nh', 'ƒë√†o t·∫°o'],
            'general': ['th√¥ng tin', 'h·ªó tr·ª£', 'gi√∫p', 'h∆∞·ªõng d·∫´n']
        }
        
        # ‚úÖ EXPECTED PATTERNS cho reject/accept
        self.rejection_patterns = [
            "em ch·ªâ h·ªó tr·ª£ c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn c√¥ng vi·ªác gi·∫£ng vi√™n",
            "em ch·ªâ h·ªó tr·ª£",
            "kh√¥ng th·ªÉ h·ªó tr·ª£",
            "ngo√†i ph·∫°m vi"
        ]
        
        self.clarification_patterns = [
            "ƒë·ªÉ em h·ªó tr·ª£ ch√≠nh x√°c",
            "th·∫ßy/c√¥ c√≥ th·ªÉ n√≥i r√µ h∆°n",
            "c·∫ßn l√†m r√µ",
            "th√¥ng tin c·ª• th·ªÉ"
        ]
        
        print("üöÄ ChatBot Tester initialized!")
    
    def load_test_data(self, csv_path=None):
        """Load test data from CSV or create sample data"""
        if csv_path and os.path.exists(csv_path):
            df = pd.read_csv(csv_path, encoding='utf-8')
            test_queries = df['question'].fillna('').tolist()
            
            # Handle expected answers
            if 'answer' in df.columns:
                expected_answers = df['answer'].fillna('').tolist()
            else:
                expected_answers = ['']*len(test_queries)
            
            # Handle categories - fixed logic
            if 'category' in df.columns:
                categories = df['category'].fillna('general').tolist()
            else:
                categories = ['general']*len(test_queries)
                
            print(f"üìä Loaded from CSV: {len(test_queries)} queries")
        else:
            # ‚úÖ SAMPLE TEST DATA n·∫øu kh√¥ng c√≥ CSV
            test_queries, expected_answers, categories = self.generate_sample_test_data()
            print(f"üìä Using sample data: {len(test_queries)} queries")
        
        return test_queries, expected_answers, categories
    
    def generate_sample_test_data(self):
        """Generate comprehensive sample test data"""
        test_data = [
            # ‚úÖ EDUCATION RELATED - should be accepted
            ("h·ªçc ph√≠ c·ªßa tr∆∞·ªùng l√† bao nhi√™u?", "", "tuition_fees"),
            ("l·ªá ph√≠ nh·∫≠n b·∫±ng t·ªët nghi·ªáp l√† bao nhi√™u?", "", "graduation"),
            ("v·∫≠y c√≤n l·ªá ph√≠ thu·∫≠t l√™ ph·ª•c v·∫£ th√¨ sao?", "", "tuition_fees"),
            ("ph√≠ chuy·ªÉn kho·∫£n l√† bao nhi√™u?", "", "tuition_fees"),
            ("c√°ch th·ª©c n·ªôp h·ªçc ph√≠ nh∆∞ th·∫ø n√†o?", "", "tuition_fees"),
            ("tuy·ªÉn sinh nƒÉm 2024 c√≥ g√¨ m·ªõi?", "", "admission"),
            ("ƒëi·ªÅu ki·ªán t·ªët nghi·ªáp nh∆∞ th·∫ø n√†o?", "", "graduation"),
            ("th·ªß t·ª•c nh·∫≠n b·∫±ng c·∫ßn g√¨?", "", "graduation"),
            ("ng√¢n h√†ng ƒë·ªÅ thi n·ªôp khi n√†o?", "", "lecturer_tasks"),
            ("k√™ khai nhi·ªám v·ª• nƒÉm h·ªçc ·ªü ƒë√¢u?", "", "lecturer_tasks"),
            ("t·∫°p ch√≠ khoa h·ªçc nh·∫≠n b√†i kh√¥ng?", "", "academic_journal"),
            ("thi ƒëua khen th∆∞·ªüng nƒÉm nay th·∫ø n√†o?", "", "competition_awards"),
            ("l·ªãch gi·∫£ng d·∫°y c·∫≠p nh·∫≠t ·ªü ƒë√¢u?", "", "schedule"),
            ("ph√≤ng ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng ·ªü ƒë√¢u?", "", "departments"),
            ("ng√†nh c√¥ng ngh·ªá th√¥ng tin h·ªçc nh·ªØng g√¨?", "", "programs"),
            
            # ‚úÖ VAGUE QUESTIONS - should ask for clarification
            ("th√¥ng tin g√¨?", "", "general"),
            ("h∆∞·ªõng d·∫´n l√†m sao?", "", "general"),
            ("c√°ch n√†o ƒë·ªÉ?", "", "general"),
            ("th·ªß t·ª•c nh∆∞ th·∫ø n√†o?", "", "general"),
            
            # ‚úÖ NON-EDUCATION - should be rejected (test carefully)
            ("th·ªùi ti·∫øt h√¥m nay th·∫ø n√†o?", "", "non_education"),
            ("m√≥n ƒÉn ngon ·ªü ƒë√¢u?", "", "non_education"),
            ("c√°ch n·∫•u ph·ªü nh∆∞ th·∫ø n√†o?", "", "non_education"),
            
            # ‚úÖ EDGE CASES
            ("", "", "empty"),
            ("bdu", "", "general"),
            ("ƒë·∫°i h·ªçc b√¨nh d∆∞∆°ng", "", "general"),
            ("gi·∫£ng vi√™n", "", "general"),
            ("th·∫ßy c√¥", "", "general"),
            
            # ‚úÖ FOLLOW-UP QUESTIONS (test memory)
            ("c√≤n v·ªÅ ph√≠ kh√°c th√¨ sao?", "", "tuition_fees"),
            ("v·∫≠y th√™m th√¥ng tin g√¨ n·ªØa?", "", "general"),
            ("chi ti·∫øt h∆°n v·ªÅ v·∫•n ƒë·ªÅ n√†y?", "", "general"),
            
            # ‚úÖ MIXED LANGUAGE
            ("h·ªçc ph√≠ tuition fee l√† bao nhi√™u?", "", "tuition_fees"),
            ("admission requirements ƒëi·ªÅu ki·ªán g√¨?", "", "admission"),
            
            # ‚úÖ TYPOS & INFORMAL
            ("hoc phi la bao nhieu?", "", "tuition_fees"),
            ("t√¥t nghiep can gi?", "", "graduation"),
            ("h√™ th·ªëng c√≥ h·ªó tr·ª£ kh√¥ng?", "", "general"),
        ]
        
        queries, answers, cats = zip(*test_data)
        return list(queries), list(answers), list(cats)
    
    def categorize_query(self, query):
        """Automatically categorize query based on keywords"""
        query_lower = query.lower()
        
        for category, keywords in self.categories.items():
            if any(keyword in query_lower for keyword in keywords):
                return category
        return 'unknown'
    
    def analyze_response(self, query, response_data):
        """Analyze chatbot response and categorize result"""
        response_text = response_data.get('response', '')
        confidence = response_data.get('confidence', 0)
        method = response_data.get('method', '')
        decision_type = response_data.get('decision_type', '')
        
        # ‚úÖ CLASSIFY RESPONSE TYPE
        if any(pattern in response_text.lower() for pattern in self.rejection_patterns):
            response_type = 'rejected'
        elif any(pattern in response_text.lower() for pattern in self.clarification_patterns):
            response_type = 'clarification'
        elif 'em ch∆∞a c√≥ th√¥ng tin' in response_text.lower():
            response_type = 'no_info'
        elif 'd·∫° th·∫ßy/c√¥' in response_text.lower():
            response_type = 'answered'
        else:
            response_type = 'unknown'
        
        return {
            'response_type': response_type,
            'confidence': confidence,
            'method': method,
            'decision_type': decision_type,
            'response_length': len(response_text),
            'has_emoji': 'üéì' in response_text or 'üìö' in response_text,
            'proper_addressing': 'th·∫ßy/c√¥' in response_text.lower()
        }
    
    def test_single_query(self, query, expected_category=None):
        """Test a single query and return detailed results"""
        print(f"\nüîç Testing: '{query[:50]}{'...' if len(query) > 50 else ''}'")
        
        start_time = time.time()
        
        try:
            # Skip empty queries
            if not query or not query.strip():
                result = {
                    'query': query,
                    'test_result': 'SKIPPED - Empty query',
                    'timestamp': datetime.now().isoformat()
                }
                print(f"   ‚è≠Ô∏è  SKIPPED - Empty query")
                self.test_results.append(result)
                return result
            
            # Get chatbot response
            response_data = self.chatbot.process_query(query, session_id=self.session_id)
            
            # Analyze response
            analysis = self.analyze_response(query, response_data)
            
            # Categorize query
            detected_category = self.categorize_query(query)
            
            # Check if query should be education-related
            is_education_query = self.chatbot.decision_engine.is_education_related(query)
            
            result = {
                'query': query,
                'expected_category': expected_category or detected_category,
                'detected_category': detected_category,
                'is_education_query': is_education_query,
                'response': response_data.get('response', ''),
                'confidence': response_data.get('confidence', 0),
                'method': response_data.get('method', ''),
                'decision_type': response_data.get('decision_type', ''),
                'processing_time': time.time() - start_time,
                'analysis': analysis,
                'timestamp': datetime.now().isoformat()
            }
            
            # ‚úÖ CLASSIFY SUCCESS/FAILURE
            if analysis['response_type'] == 'rejected' and is_education_query:
                result['test_result'] = 'FAILED - Education query was rejected'
                self.failed_queries.append(result)
            elif analysis['response_type'] == 'answered' and not is_education_query:
                result['test_result'] = 'WARNING - Non-education query was answered'
            elif analysis['response_type'] == 'unknown':
                result['test_result'] = 'FAILED - Unknown response type'
                self.failed_queries.append(result)
            else:
                result['test_result'] = 'PASSED'
            
            print(f"   üìä Result: {result['test_result']}")
            print(f"   üìà Confidence: {result['confidence']:.3f}")
            print(f"   ü§ñ Response Type: {analysis['response_type']}")
            print(f"   ‚ö° Time: {result['processing_time']:.3f}s")
            
        except Exception as e:
            result = {
                'query': query,
                'test_result': f'ERROR - {str(e)}',
                'error': str(e),
                'processing_time': time.time() - start_time,
                'timestamp': datetime.now().isoformat()
            }
            print(f"   ‚ùå ERROR: {str(e)}")
        
        self.test_results.append(result)
        return result
    
    def run_comprehensive_test(self, csv_path=None, output_dir='test_results'):
        """Run comprehensive test suite"""
        print("üöÄ Starting Comprehensive ChatBot Test...")
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        # Load test data
        test_queries, expected_answers, categories = self.load_test_data(csv_path)
        
        print(f"üìù Loaded {len(test_queries)} test queries")
        
        # Run tests
        for i, (query, expected, category) in enumerate(zip(test_queries, expected_answers, categories)):
            print(f"\n[{i+1}/{len(test_queries)}]", end="")
            self.test_single_query(query, category)
            
            # Small delay to avoid overwhelming
            time.sleep(0.1)
        
        # Generate comprehensive report
        self.generate_comprehensive_report(output_dir)
        
        print(f"\n‚úÖ Test completed! Results saved to {output_dir}/")
    
    def generate_comprehensive_report(self, output_dir):
        """Generate comprehensive test report with analysis"""
        
        # ‚úÖ 1. SUMMARY STATISTICS
        total_tests = len(self.test_results)
        passed_tests = len([r for r in self.test_results if r.get('test_result', '').startswith('PASSED')])
        failed_tests = len([r for r in self.test_results if r.get('test_result', '').startswith('FAILED')])
        error_tests = len([r for r in self.test_results if r.get('test_result', '').startswith('ERROR')])
        warning_tests = len([r for r in self.test_results if r.get('test_result', '').startswith('WARNING')])
        
        # ‚úÖ 2. RESPONSE TYPE ANALYSIS
        response_types = Counter([r.get('analysis', {}).get('response_type', 'unknown') for r in self.test_results])
        
        # ‚úÖ 3. CONFIDENCE ANALYSIS
        confidences = [r.get('confidence', 0) for r in self.test_results if 'confidence' in r]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0
        
        # ‚úÖ 4. FAILED QUERIES ANALYSIS
        failed_education_queries = []
        rejected_education_queries = []
        
        for result in self.test_results:
            if result.get('is_education_query', False):
                analysis = result.get('analysis', {})
                if analysis.get('response_type') == 'rejected':
                    rejected_education_queries.append(result)
                elif result.get('test_result', '').startswith('FAILED'):
                    failed_education_queries.append(result)
        
        # ‚úÖ 5. KEYWORD GAP ANALYSIS
        missing_keywords = self.analyze_missing_keywords()
        
        # ‚úÖ 6. GENERATE REPORTS
        
        # Summary Report
        summary_report = {
            'test_summary': {
                'total_tests': total_tests,
                'passed': passed_tests,
                'failed': failed_tests,
                'errors': error_tests,
                'warnings': warning_tests,
                'pass_rate': f"{(passed_tests/total_tests*100):.1f}%" if total_tests > 0 else "0%"
            },
            'response_analysis': {
                'response_types': dict(response_types),
                'average_confidence': f"{avg_confidence:.3f}",
                'total_rejected_education_queries': len(rejected_education_queries),
                'total_failed_education_queries': len(failed_education_queries)
            },
            'keyword_gaps': missing_keywords,
            'recommendations': self.generate_recommendations()
        }
        
        # Save summary
        with open(f"{output_dir}/test_summary.json", 'w', encoding='utf-8') as f:
            json.dump(summary_report, f, ensure_ascii=False, indent=2)
        
        # Save detailed results
        df_results = pd.DataFrame(self.test_results)
        df_results.to_csv(f"{output_dir}/detailed_results.csv", encoding='utf-8', index=False)
        
        # Save failed queries for analysis
        if failed_education_queries or rejected_education_queries:
            failed_df = pd.DataFrame(rejected_education_queries + failed_education_queries)
            failed_df.to_csv(f"{output_dir}/failed_education_queries.csv", encoding='utf-8', index=False)
        
        # ‚úÖ 7. PRINT SUMMARY TO CONSOLE
        print("\n" + "="*80)
        print("üìä COMPREHENSIVE TEST REPORT")
        print("="*80)
        print(f"üìà Total Tests: {total_tests}")
        print(f"‚úÖ Passed: {passed_tests} ({(passed_tests/total_tests*100):.1f}%)")
        print(f"‚ùå Failed: {failed_tests}")
        print(f"‚ö†Ô∏è  Warnings: {warning_tests}")
        print(f"üî• Errors: {error_tests}")
        print(f"üìä Average Confidence: {avg_confidence:.3f}")
        
        print(f"\nü§ñ Response Types:")
        for resp_type, count in response_types.most_common():
            print(f"   {resp_type}: {count}")
        
        print(f"\n‚ùå Education Queries Rejected: {len(rejected_education_queries)}")
        if rejected_education_queries:
            print("   Top rejected education queries:")
            for result in rejected_education_queries[:5]:
                print(f"   - '{result['query']}'")
        
        if missing_keywords:
            print(f"\nüîç Missing Keywords Found: {len(missing_keywords)}")
            for keyword in missing_keywords[:10]:
                print(f"   - '{keyword}'")
        
        print(f"\nüìÅ Full reports saved to: {output_dir}/")
        print("="*80)
    
    def analyze_missing_keywords(self):
        """Analyze queries to find missing keywords"""
        missing_keywords = []
        
        # Analyze rejected education queries
        for result in self.test_results:
            if (result.get('is_education_query', False) and 
                result.get('analysis', {}).get('response_type') == 'rejected'):
                
                query = result['query'].lower()
                
                # Extract potential keywords
                words = re.findall(r'\b\w+\b', query)
                
                # Filter out common words
                common_words = {'l√†', 'c·ªßa', 'c√≥', 'th·ªÉ', 'ƒë∆∞·ª£c', 'nh∆∞', 'th·∫ø', 'n√†o', 'g√¨', 'sao', 'v√†', 'v·ªõi', 'trong', 'v·ªÅ', 'ƒë·ªÉ', 'khi', 'ƒë√£', 's·∫Ω', 'c√°c', 'm·ªôt', 'nh·ªØng'}
                
                potential_keywords = [w for w in words if len(w) > 2 and w not in common_words]
                
                # Check if any of these words are in current keywords
                current_keywords = self.chatbot.decision_engine.education_keywords
                for word in potential_keywords:
                    if word not in current_keywords and word not in missing_keywords:
                        missing_keywords.append(word)
        
        return missing_keywords[:20]  # Top 20 missing keywords
    
    def generate_recommendations(self):
        """Generate actionable recommendations"""
        recommendations = []
        
        # Check pass rate
        total_tests = len(self.test_results)
        passed_tests = len([r for r in self.test_results if r.get('test_result', '').startswith('PASSED')])
        pass_rate = (passed_tests/total_tests) if total_tests > 0 else 0
        
        if pass_rate < 0.8:
            recommendations.append("üî¥ Pass rate is below 80%. Consider reviewing education keyword detection logic.")
        
        # Check rejected education queries
        rejected_education = len([r for r in self.test_results 
                                if r.get('is_education_query', False) and 
                                   r.get('analysis', {}).get('response_type') == 'rejected'])
        
        if rejected_education > 0:
            recommendations.append(f"üî¥ {rejected_education} education queries were rejected. Review education_keywords list.")
        
        # Check confidence levels
        low_confidence_queries = len([r for r in self.test_results if r.get('confidence', 0) < 0.3])
        
        if low_confidence_queries > total_tests * 0.3:
            recommendations.append("üü° High number of low-confidence responses. Consider improving knowledge base or similarity thresholds.")
        
        # Check processing time
        slow_queries = len([r for r in self.test_results if r.get('processing_time', 0) > 2.0])
        
        if slow_queries > 0:
            recommendations.append(f"üü° {slow_queries} queries took longer than 2 seconds. Consider performance optimization.")
        
        if not recommendations:
            recommendations.append("‚úÖ All metrics look good! System is performing well.")
        
        return recommendations
    
    def test_memory_functionality(self):
        """Test conversation memory functionality"""
        print("\nüß† Testing Memory Functionality...")
        
        memory_test_session = f"memory_test_{int(time.time())}"
        
        # Test sequence
        queries = [
            "h·ªçc ph√≠ c·ªßa tr∆∞·ªùng l√† bao nhi√™u?",
            "v·∫≠y c√≤n l·ªá ph√≠ kh√°c th√¨ sao?",
            "chi ti·∫øt h∆°n v·ªÅ ph√≠ n√†y?",
            "c·∫£m ∆°n th√¥ng tin"
        ]
        
        for i, query in enumerate(queries):
            print(f"\n[Memory Test {i+1}] {query}")
            result = self.chatbot.process_query(query, session_id=memory_test_session)
            
            print(f"Response: {result['response'][:100]}...")
            print(f"Decision: {result.get('decision_type', 'unknown')}")
            
            # Check memory
            memory = self.chatbot.get_conversation_context(memory_test_session)
            print(f"Memory entries: {len(memory)}")
    
    def run_specific_category_test(self, category):
        """Test specific category of queries"""
        print(f"\nüéØ Testing {category} category...")
        
        if category in self.categories:
            keywords = self.categories[category]
            
            # Generate test queries for this category
            test_queries = [
                f"{keyword} l√† g√¨?",
                f"th√¥ng tin v·ªÅ {keyword}",
                f"h∆∞·ªõng d·∫´n {keyword}",
                f"c√°ch th·ª©c {keyword} nh∆∞ th·∫ø n√†o?"
            ]
            
            for query in test_queries:
                self.test_single_query(query, category)


def main():
    """Main execution function"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ChatBot Testing Script')
    parser.add_argument('--csv', help='Path to CSV file with test queries')
    parser.add_argument('--output', default='test_results', help='Output directory for results')
    parser.add_argument('--category', help='Test specific category only')
    parser.add_argument('--memory', action='store_true', help='Test memory functionality')
    
    args = parser.parse_args()
    
    tester = ChatBotTester()
    
    if args.memory:
        tester.test_memory_functionality()
    elif args.category:
        tester.run_specific_category_test(args.category)
    else:
        tester.run_comprehensive_test(csv_path=args.csv, output_dir=args.output)


if __name__ == "__main__":
    main()