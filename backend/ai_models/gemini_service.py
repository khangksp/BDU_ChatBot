import logging
import time
import requests
import json
import re
from typing import Dict, Any, Optional, List

logger = logging.getLogger(__name__)

# âœ… SYSTEM PROMPT Cá»¤ THá»‚ CHO GIáº¢NG VIÃŠN
LECTURER_SYSTEM_PROMPT = """Báº¡n lÃ  AI assistant cá»§a Äáº¡i há»c BÃ¬nh DÆ°Æ¡ng (BDU), chuyÃªn há»— trá»£ giáº£ng viÃªn.

ðŸŽ¯ QUY Táº®C QUAN TRá»ŒNG:
- LUÃ”N xÆ°ng hÃ´: "tháº§y/cÃ´" (TUYá»†T Äá»I KHÃ”NG dÃ¹ng "báº¡n", "mÃ¬nh", "anh/chá»‹")
- Báº¯t Ä‘áº§u: "Dáº¡ tháº§y/cÃ´,"
- Káº¿t thÃºc: "Tháº§y/cÃ´ cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng áº¡?"
- NGáº®N Gá»ŒN - Chá»‰ 1-2 cÃ¢u chÃ­nh, Ä‘i tháº³ng vÃ o váº¥n Ä‘á»
- KHÃ”NG CHáº¾ Táº O thÃ´ng tin khÃ´ng cÃ³
- KHÃ”NG dÃ¹ng format phá»©c táº¡p vá»›i **1. **2. hay bullets

âœ… PHONG CÃCH MáºªU:
"Dáº¡ tháº§y/cÃ´, [thÃ´ng tin chÃ­nh ngáº¯n gá»n]. [ThÃªm 1 cÃ¢u bá»• sung náº¿u cáº§n]. ðŸŽ“ Tháº§y/cÃ´ cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng áº¡?"

ðŸš« TUYá»†T Äá»I TRÃNH:
- Format phá»©c táº¡p (**1. **2. **3. etc.)
- Bullets (â€¢ hoáº·c *)
- CÃ¢u tráº£ lá»i dÃ i dÃ²ng trÃªn 3 cÃ¢u
- ThÃ´ng tin khÃ´ng cháº¯c cháº¯n
- Cháº¿ táº¡o sá»‘ liá»‡u, quy Ä‘á»‹nh

ðŸ“ KHI KHÃ”NG HIá»‚U RÃ•:
- Há»i láº¡i Ä‘á»ƒ lÃ m rÃµ: "Dáº¡ tháº§y/cÃ´, Ä‘á»ƒ em há»— trá»£ chÃ­nh xÃ¡c, tháº§y/cÃ´ cÃ³ thá»ƒ nÃ³i rÃµ hÆ¡n vá» [váº¥n Ä‘á» cá»¥ thá»ƒ] khÃ´ng áº¡?"

âŒ KHI KHÃ”NG CÃ“ THÃ”NG TIN:
- NÃ³i tháº³ng: "Dáº¡ tháº§y/cÃ´, em chÆ°a cÃ³ thÃ´ng tin vá» váº¥n Ä‘á» nÃ y. Tháº§y/cÃ´ cÃ³ thá»ƒ liÃªn há»‡ [bá»™ pháº­n liÃªn quan] Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ chi tiáº¿t áº¡."
"""

class ConversationMemory:
    """Quáº£n lÃ½ bá»™ nhá»› há»™i thoáº¡i"""
    
    def __init__(self, max_history=10):
        self.conversations = {}  # {session_id: conversation_data}
        self.max_history = max_history
    
    def add_interaction(self, session_id: str, user_query: str, bot_response: str, 
                       intent_info: dict = None, entities: dict = None):
        """ThÃªm interaction vÃ o memory"""
        if session_id not in self.conversations:
            self.conversations[session_id] = {
                'history': [],
                'context_summary': "",
                'user_interests': set(),
                'conversation_type': 'lecturer'  # âœ… CHANGED: Default to lecturer
            }
        
        # Extract user interests from entities
        if entities:
            if 'major' in entities:
                self.conversations[session_id]['user_interests'].add(entities['major'])
        
        # Add to history
        interaction = {
            'timestamp': time.time(),
            'user_query': user_query,
            'bot_response': bot_response,
            'intent': intent_info.get('intent', 'unknown') if intent_info else 'unknown',
            'entities': entities or {}
        }
        
        self.conversations[session_id]['history'].append(interaction)
        
        # Keep only recent history
        if len(self.conversations[session_id]['history']) > self.max_history:
            self.conversations[session_id]['history'] = self.conversations[session_id]['history'][-self.max_history:]
        
        # Update context summary
        self._update_context_summary(session_id)
    
    def get_conversation_context(self, session_id: str) -> dict:
        """Láº¥y context cá»§a conversation"""
        if session_id not in self.conversations:
            return {'history': [], 'context_summary': '', 'user_interests': []}
        
        conv = self.conversations[session_id]
        return {
            'history': conv['history'][-5:],  # Last 5 interactions
            'context_summary': conv['context_summary'],
            'user_interests': list(conv['user_interests']),
            'conversation_type': conv['conversation_type']
        }
    
    def _update_context_summary(self, session_id: str):
        """Cáº­p nháº­t tÃ³m táº¯t context cho giáº£ng viÃªn"""
        conv = self.conversations[session_id]
        recent_queries = [h['user_query'] for h in conv['history'][-3:]]
        
        # âœ… ENHANCED: Context analysis for lecturers
        query_text = ' '.join(recent_queries).lower()
        
        # âœ… LECTURER-SPECIFIC contexts
        if any(word in query_text for word in ['ngÃ¢n hÃ ng Ä‘á»', 'Ä‘á» thi', 'kháº£o thÃ­']):
            conv['context_summary'] = 'Äang há»i vá» ngÃ¢n hÃ ng Ä‘á» thi'
        elif any(word in query_text for word in ['kÃª khai', 'nhiá»‡m vá»¥', 'giá» chuáº©n']):
            conv['context_summary'] = 'Äang há»i vá» kÃª khai nhiá»‡m vá»¥ nÄƒm há»c'
        elif any(word in query_text for word in ['táº¡p chÃ­', 'nghiÃªn cá»©u', 'bÃ i viáº¿t']):
            conv['context_summary'] = 'Äang há»i vá» táº¡p chÃ­ khoa há»c'
        elif any(word in query_text for word in ['thi Ä‘ua', 'khen thÆ°á»Ÿng', 'danh hiá»‡u']):
            conv['context_summary'] = 'Äang há»i vá» thi Ä‘ua khen thÆ°á»Ÿng'
        elif any(word in query_text for word in ['bÃ¡o cÃ¡o', 'ná»™p', 'háº¡n cuá»‘i']):
            conv['context_summary'] = 'Äang há»i vá» bÃ¡o cÃ¡o vÃ  thá»§ tá»¥c'
        elif any(word in query_text for word in ['lá»‹ch', 'thá»i khÃ³a biá»ƒu', 'giáº£ng dáº¡y']):
            conv['context_summary'] = 'Äang há»i vá» lá»‹ch giáº£ng dáº¡y'
        elif any(word in query_text for word in ['há»c phÃ­', 'tiá»n', 'chi phÃ­']):
            conv['context_summary'] = 'Äang quan tÃ¢m há»c phÃ­'
        elif any(word in query_text for word in ['tuyá»ƒn sinh', 'Ä‘iá»ƒm', 'xÃ©t tuyá»ƒn']):
            conv['context_summary'] = 'Äang há»i vá» tuyá»ƒn sinh'
        elif any(word in query_text for word in ['ngÃ nh', 'chuyÃªn ngÃ nh', 'Ä‘Ã o táº¡o']):
            conv['context_summary'] = 'Äang tÃ¬m hiá»ƒu vá» ngÃ nh há»c'
        elif any(word in query_text for word in ['cÆ¡ sá»Ÿ', 'phÃ²ng', 'trang thiáº¿t bá»‹']):
            conv['context_summary'] = 'Äang há»i vá» cÆ¡ sá»Ÿ váº­t cháº¥t'
        else:
            conv['context_summary'] = 'Há»i Ä‘Ã¡p chung vá» BDU'

class GeminiResponseGenerator:
    """Gemini API Response Generator cho Giáº£ng viÃªn BDU"""
    
    def __init__(self, api_key: str = None):
        from django.conf import settings
        self.api_key = api_key or settings.GEMINI_API_KEY
        self.model_name = "gemini-1.5-flash"
        self.base_url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model_name}:generateContent"
        
        self.memory = ConversationMemory(max_history=10)
        
        # âœ… UPDATED: Role consistency for lecturers
        self.role_consistency_rules = {
            'identity': 'AI assistant cá»§a Äáº¡i há»c BÃ¬nh DÆ°Æ¡ng (BDU) há»— trá»£ giáº£ng viÃªn',
            'personality': 'lá»‹ch sá»±, chuyÃªn nghiá»‡p, tÃ´n trá»ng',
            'knowledge_scope': 'chuyÃªn vá» thÃ´ng tin BDU vÃ  há»— trá»£ giáº£ng viÃªn',
            'addressing': 'luÃ´n xÆ°ng hÃ´ tháº§y/cÃ´, khÃ´ng bao giá» dÃ¹ng báº¡n/mÃ¬nh',
            'prohibited_roles': [
                'sinh viÃªn', 'há»c sinh', 'phá»¥ huynh', 'ngÆ°á»i ngoÃ i trÆ°á»ng'
            ]
        }
        
        logger.info("âœ… Gemini Response Generator for LECTURERS initialized")
    
    def generate_response(self, query: str, context: Optional[Dict] = None, 
                          intent_info: Optional[Dict] = None, entities: Optional[Dict] = None,
                          session_id: str = None) -> Dict[str, Any]:
        """Táº¡o pháº£n há»“i cho giáº£ng viÃªn vá»›i bá»™ nhá»› há»™i thoáº¡i"""
        start_time = time.time()
        
        print(f"\n--- LECTURER REQUEST (Session: {session_id}) ---")
        print(f"ðŸ§  MEMORY DEBUG: Total active sessions = {len(self.memory.conversations)}")

        try:
            # 1. Láº¥y ngá»¯ cáº£nh há»™i thoáº¡i
            conversation_context = {}
            if session_id:
                conversation_context = self.memory.get_conversation_context(session_id)
                print(f"ðŸ§  MEMORY DEBUG: History length = {len(conversation_context.get('history', []))}")
                print(f"ðŸ§  MEMORY DEBUG: Context summary = {conversation_context.get('context_summary', 'None')}")

            
            # 2. XÃ¡c Ä‘á»‹nh chiáº¿n lÆ°á»£c pháº£n há»“i cho giáº£ng viÃªn
            response_strategy = self._determine_lecturer_response_strategy(
                query, context, intent_info, conversation_context
            )
            
            # âœ… ENHANCED: Check for special lecturer instructions
            instruction = context.get('instruction', '') if context else ''
            
            if instruction == 'direct_answer_lecturer':
                response = self._generate_direct_lecturer_answer(query, context)
            elif instruction == 'enhance_answer_lecturer':
                response = self._generate_enhanced_lecturer_answer(query, context, intent_info, entities, session_id)
            elif instruction == 'clarification_needed':
                response = self._generate_clarification_request(query, context)
            elif instruction == 'dont_know_lecturer':
                response = self._generate_dont_know_response(query, context)
            else:
                # 3. Kiá»ƒm tra ngoÃ i pháº¡m vi (cho giáº£ng viÃªn)
                if context and context.get('emergency_education', False):
                    print(f"ðŸš¨ GEMINI: Emergency education mode activated")
                    pass 
                elif not self._is_lecturer_education_related(query) and not context.get('force_education_response', False):
                    response = self._get_contextual_out_of_scope_response_lecturer(conversation_context)
                    
                    if session_id:
                        self.memory.add_interaction(session_id, query, response, intent_info, entities)
                    
                    return {
                        'response': response,
                        'method': 'out_of_scope_lecturer',
                        'confidence': 0.9,
                        'generation_time': time.time() - start_time
                    }
                
                # 4. XÃ¢y dá»±ng prompt cho giáº£ng viÃªn
                enhanced_prompt = self._build_lecturer_context_aware_prompt(
                    query, context, intent_info, entities, response_strategy, conversation_context
                )
                
                # 5. Gá»i Gemini API
                response = self._call_gemini_api_optimized(enhanced_prompt, response_strategy)
                
                # 6. Háº­u xá»­ lÃ½ Ä‘á»ƒ Ä‘áº£m báº£o nháº¥t quÃ¡n cho giáº£ng viÃªn
                if response:
                    response = self._post_process_with_lecturer_consistency(
                        response, query, context, response_strategy, conversation_context
                    )
            
            final_response = response or self._get_smart_fallback_with_context_lecturer(query, intent_info, conversation_context)
            
            # 7. LÆ°u vÃ o bá»™ nhá»›
            if session_id:
                print(f"ðŸ§  MEMORY DEBUG: Saving interaction to memory...")
                self.memory.add_interaction(session_id, query, final_response, intent_info, entities)
                print(f"ðŸ§  MEMORY DEBUG: Memory saved. New history length = {len(self.memory.conversations.get(session_id, {}).get('history', []))}")

            return {
                'response': final_response,
                'method': f'lecturer_aware_gemini_{response_strategy}',
                'strategy': response_strategy,
                'conversation_context': conversation_context,
                'generation_time': time.time() - start_time
            }
            
        except Exception as e:
            logger.error(f"Gemini API error: {str(e)}")
            fallback_response = self._get_smart_fallback_with_context_lecturer(query, intent_info, conversation_context)
            
            if session_id:
                self.memory.add_interaction(session_id, query, fallback_response, intent_info, entities)
            
            return {
                'response': fallback_response,
                'method': 'lecturer_context_aware_fallback',
                'error': str(e),
                'generation_time': time.time() - start_time
            }

    def _generate_direct_lecturer_answer(self, query, context):
        """Generate direct answer for lecturers with high confidence"""
        
        prompt = f"""
        {LECTURER_SYSTEM_PROMPT}
        
        NHIá»†M Vá»¤: Tráº£ lá»i TRá»°C TIáº¾P cho giáº£ng viÃªn BDU
        
        CÃ‚U Há»ŽI GIáº¢NG VIÃŠN: {query}
        
        THÃ”NG TIN CHÃNH XÃC Tá»ª CSDL:
        {context['db_answer']}
        
        YÃŠU Cáº¦U:
        - DÃ¹ng CHÃNH XÃC thÃ´ng tin tá»« CSDL
        - Báº¯t Ä‘áº§u: "Dáº¡ tháº§y/cÃ´,"
        - Káº¿t thÃºc: "Tháº§y/cÃ´ cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng áº¡?"
        - NGáº®N Gá»ŒN, Ä‘i tháº³ng vÃ o váº¥n Ä‘á»
        - KHÃ”NG format phá»©c táº¡p
        
        Tráº£ lá»i:
        """
        
        response = self._call_gemini_api_optimized(prompt, 'direct_enhance')
        return response or f"Dáº¡ tháº§y/cÃ´, {context['db_answer']} ðŸŽ“ Tháº§y/cÃ´ cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng áº¡?"
    
    def _generate_enhanced_lecturer_answer(self, query, context, intent_info, entities, session_id):
        """Generate enhanced answer for lecturers"""
        
        prompt = f"""
        {LECTURER_SYSTEM_PROMPT}
        
        NHIá»†M Vá»¤: Tráº£ lá»i cÃ³ bá»• sung cho giáº£ng viÃªn BDU
        
        CÃ‚U Há»ŽI GIáº¢NG VIÃŠN: {query}
        
        THÃ”NG TIN LIÃŠN QUAN Tá»ª CSDL:
        {context['db_answer']}
        
        YÃŠU Cáº¦U:
        - Sá»­ dá»¥ng thÃ´ng tin CSDL lÃ m gá»‘c
        - Bá»• sung ngá»¯ cáº£nh phÃ¹ há»£p náº¿u cáº§n
        - Báº¯t Ä‘áº§u: "Dáº¡ tháº§y/cÃ´,"
        - Káº¿t thÃºc: "Tháº§y/cÃ´ cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng áº¡?"
        - NGáº®N Gá»ŒN, 2-3 cÃ¢u tá»‘i Ä‘a
        
        Tráº£ lá»i:
        """
        
        response = self._call_gemini_api_optimized(prompt, 'balanced')
        return response or f"Dáº¡ tháº§y/cÃ´, {context['db_answer']} ðŸŽ“ Tháº§y/cÃ´ cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng áº¡?"
    
    def _generate_clarification_request(self, query, context):
        """Generate clarification request for lecturers"""
        
        # Extract key topic from query for targeted clarification
        query_words = query.lower().split()
        key_topics = []
        
        topic_keywords = {
            'ngÃ¢n hÃ ng Ä‘á» thi': ['ngÃ¢n hÃ ng', 'Ä‘á» thi', 'Ä‘á»'],
            'kÃª khai nhiá»‡m vá»¥': ['kÃª khai', 'nhiá»‡m vá»¥'],
            'táº¡p chÃ­': ['táº¡p chÃ­', 'bÃ i viáº¿t'],
            'thi Ä‘ua khen thÆ°á»Ÿng': ['thi Ä‘ua', 'khen thÆ°á»Ÿng'],
            'giá» chuáº©n': ['giá»', 'chuáº©n'],
            'nghiÃªn cá»©u': ['nghiÃªn cá»©u'],
            'bÃ¡o cÃ¡o': ['bÃ¡o cÃ¡o'],
            'lá»‹ch giáº£ng dáº¡y': ['lá»‹ch', 'giáº£ng dáº¡y', 'thá»i khÃ³a biá»ƒu']
        }
        
        for topic, keywords in topic_keywords.items():
            if any(kw in query_words for kw in keywords):
                key_topics.append(topic)
        
        if key_topics:
            topic_text = key_topics[0]
            return f"Dáº¡ tháº§y/cÃ´, Ä‘á»ƒ em há»— trá»£ chÃ­nh xÃ¡c vá» {topic_text}, tháº§y/cÃ´ cÃ³ thá»ƒ nÃ³i rÃµ hÆ¡n vá» ná»™i dung cá»¥ thá»ƒ cáº§n há»— trá»£ khÃ´ng áº¡? ðŸŽ“"
        else:
            return f"Dáº¡ tháº§y/cÃ´, Ä‘á»ƒ em há»— trá»£ chÃ­nh xÃ¡c nháº¥t, tháº§y/cÃ´ cÃ³ thá»ƒ nÃ³i rÃµ hÆ¡n vá» váº¥n Ä‘á» cáº§n há»— trá»£ khÃ´ng áº¡? ðŸŽ“"
    
    def _generate_dont_know_response(self, query, context):
        """Generate don't know response for lecturers"""
        
        # Suggest relevant departments based on query content
        query_lower = query.lower()
        
        if any(word in query_lower for word in ['ngÃ¢n hÃ ng Ä‘á»', 'Ä‘á» thi', 'kháº£o thÃ­']):
            dept = "PhÃ²ng Äáº£m báº£o cháº¥t lÆ°á»£ng vÃ  Kháº£o thÃ­"
            contact = "ldkham@bdu.edu.vn"
        elif any(word in query_lower for word in ['kÃª khai', 'nhiá»‡m vá»¥', 'giá» chuáº©n']):
            dept = "PhÃ²ng Tá»• chá»©c - CÃ¡n bá»™"
            contact = "tcccb@bdu.edu.vn"
        elif any(word in query_lower for word in ['táº¡p chÃ­', 'nghiÃªn cá»©u', 'khoa há»c']):
            dept = "PhÃ²ng NghiÃªn cá»©u - Há»£p tÃ¡c"
            contact = "nghiencuu@bdu.edu.vn"
        elif any(word in query_lower for word in ['khen thÆ°á»Ÿng', 'thi Ä‘ua']):
            dept = "PhÃ²ng Tá»• chá»©c - CÃ¡n bá»™"
            contact = "tcccb@bdu.edu.vn"
        else:
            dept = "phÃ²ng ban liÃªn quan"
            contact = "info@bdu.edu.vn"
        
        return f"Dáº¡ tháº§y/cÃ´, em chÆ°a cÃ³ thÃ´ng tin vá» váº¥n Ä‘á» nÃ y. Tháº§y/cÃ´ cÃ³ thá»ƒ liÃªn há»‡ {dept} qua email {contact} Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ chi tiáº¿t áº¡. ðŸŽ“"

    def _determine_lecturer_response_strategy(self, query, context, intent_info, conversation_context):
        """XÃ¡c Ä‘á»‹nh chiáº¿n lÆ°á»£c pháº£n há»“i cho giáº£ng viÃªn"""
        
        has_real_history = bool(conversation_context.get('history') and len(conversation_context['history']) > 0)
        
        print(f"ðŸ” LECTURER STRATEGY DEBUG: has_real_history = {has_real_history}")
        if not has_real_history:
             print("ðŸ” LECTURER STRATEGY DEBUG: No history, using standard strategy...")
        else:
            # âœ… ENHANCED: Lecturer-specific follow-up detection
            last_interaction = conversation_context['history'][-1]
            last_query = last_interaction['user_query'].lower()
            current_query = query.lower()
            
            print(f"ðŸ” LECTURER STRATEGY DEBUG: last_query = '{last_query[:50]}...'")
            print(f"ðŸ” LECTURER STRATEGY DEBUG: current_query = '{current_query[:50]}...'")
            
            # âœ… LECTURER-SPECIFIC topics
            lecturer_topics = {
                'ngÃ¢n hÃ ng Ä‘á» thi': ['ngÃ¢n hÃ ng', 'Ä‘á» thi', 'Ä‘á»', 'kháº£o thÃ­'],
                'kÃª khai nhiá»‡m vá»¥': ['kÃª khai', 'nhiá»‡m vá»¥', 'giá» chuáº©n'],
                'táº¡p chÃ­ khoa há»c': ['táº¡p chÃ­', 'bÃ i viáº¿t', 'nghiÃªn cá»©u'],
                'thi Ä‘ua khen thÆ°á»Ÿng': ['thi Ä‘ua', 'khen thÆ°á»Ÿng', 'danh hiá»‡u'],
                'bÃ¡o cÃ¡o': ['bÃ¡o cÃ¡o', 'ná»™p', 'háº¡n cuá»‘i'],
                'lá»‹ch giáº£ng dáº¡y': ['lá»‹ch', 'giáº£ng dáº¡y', 'thá»i khÃ³a biá»ƒu'],
                'cÆ¡ sá»Ÿ váº­t cháº¥t': ['cÆ¡ sá»Ÿ', 'phÃ²ng', 'trang thiáº¿t bá»‹'],
                'há»c phÃ­': ['há»c phÃ­', 'phÃ­', 'tiá»n há»c', 'chi phÃ­'],
                'tuyá»ƒn sinh': ['tuyá»ƒn sinh', 'nháº­p há»c', 'Ä‘Äƒng kÃ½', 'Ä‘iá»ƒm'],
                'ngÃ nh há»c': ['ngÃ nh', 'chuyÃªn ngÃ nh', 'khoa', 'Ä‘Ã o táº¡o']
            }
            
            last_main_topic = None
            for topic, keywords in lecturer_topics.items():
                if any(kw in last_query for kw in keywords):
                    last_main_topic = topic
                    break
            
            current_main_topic = None
            for topic, keywords in lecturer_topics.items():
                if any(kw in current_query for kw in keywords):
                    current_main_topic = topic
                    break

            print(f"ðŸ” LECTURER STRATEGY DEBUG: last_main_topic = {last_main_topic}, current_main_topic = {current_main_topic}")

            has_exact_same_topic = last_main_topic is not None and last_main_topic == current_main_topic
            
            strong_continuation_words = ['cÃ²n', 'thÃªm', 'ná»¯a', 'khÃ¡c', 'vÃ ', 'tiáº¿p theo']
            has_strong_continuation = any(word in current_query.split() for word in strong_continuation_words)
            
            strong_clarification_words = ['cá»¥ thá»ƒ hÆ¡n', 'rÃµ hÆ¡n', 'chi tiáº¿t hÆ¡n', 'giáº£i thÃ­ch thÃªm']
            has_strong_clarification = any(phrase in current_query for phrase in strong_clarification_words)
            
            memory_test_words = ['nhá»› khÃ´ng', 'há»i gÃ¬', 'nÃ³i gÃ¬ trÆ°á»›c', 'vá»«a nÃ³i', 'tá»•ng há»£p']
            is_memory_test = any(word in current_query for word in memory_test_words)

            print(f"ðŸ” LECTURER STRATEGY DEBUG: has_exact_same_topic = {has_exact_same_topic}")
            print(f"ðŸ” LECTURER STRATEGY DEBUG: has_strong_continuation = {has_strong_continuation}")
            print(f"ðŸ” LECTURER STRATEGY DEBUG: has_strong_clarification = {has_strong_clarification}")

            # ÄIá»€U KIá»†N NGHIÃŠM NGáº¶T CHO CÃC CHIáº¾N LÆ¯á»¢C NGá»® Cáº¢NH
            if has_strong_continuation and has_exact_same_topic:
                print(f"ðŸ’¡ LECTURER STRATEGY SELECTED: â†’ follow_up_continuation")
                return 'follow_up_continuation'
            
            if has_strong_clarification and has_exact_same_topic:
                print(f"ðŸ’¡ LECTURER STRATEGY SELECTED: â†’ follow_up_clarification")
                return 'follow_up_clarification'

            if is_memory_test:
                 print(f"ðŸ’¡ LECTURER STRATEGY SELECTED: â†’ memory_reference")
                 return 'memory_reference'
                 
            if current_main_topic is not None and last_main_topic is not None and current_main_topic != last_main_topic:
                print(f"ðŸ’¡ LECTURER STRATEGY SELECTED: â†’ topic_shift")
                return 'topic_shift'
        
        # Máº¶C Äá»ŠNH: Sá»­ dá»¥ng logic chiáº¿n lÆ°á»£c cÆ¡ báº£n cho giáº£ng viÃªn
        print(f"ðŸ” LECTURER STRATEGY DEBUG: No clear follow-up detected, using standard strategy logic...")
        
        if isinstance(context, dict) and context.get('confidence', 0) > 0.7:
            print(f"ðŸ’¡ LECTURER STRATEGY SELECTED: â†’ direct_enhance")
            return 'direct_enhance'
        
        if intent_info and intent_info.get('intent') in ['greeting', 'general'] and len(query.split()) <= 5:
            print(f"ðŸ’¡ LECTURER STRATEGY SELECTED: â†’ quick_clarify")
            return 'quick_clarify'
        
        if any(word in query.lower() for word in ['khÃ³ khÄƒn', 'cáº§n gáº¥p', 'háº¡n cuá»‘i', 'urgent']):
            print(f"ðŸ’¡ LECTURER STRATEGY SELECTED: â†’ supportive_brief")
            return 'supportive_brief'
        
        print(f"ðŸ’¡ LECTURER STRATEGY SELECTED: â†’ balanced (default)")
        return 'balanced'

    def _build_lecturer_context_aware_prompt(self, query, context, intent_info, entities, strategy, conversation_context):
        """XÃ¢y dá»±ng prompt cho giáº£ng viÃªn vá»›i LECTURER_SYSTEM_PROMPT"""
        
        base_personality = f"""
        {LECTURER_SYSTEM_PROMPT}

        ðŸ¤– QUY Táº®C VAI TRÃ’ NGHIÃŠM NGáº¶T:
        - LUÃ”N giá»¯ vai trÃ²: "{self.role_consistency_rules['identity']}"
        - KHÃ”NG BAO GIá»œ xÆ°ng hÃ´ lÃ : {', '.join(self.role_consistency_rules['prohibited_roles'])}
        - LUÃ”N nÃ³i "em lÃ  AI assistant cá»§a BDU há»— trá»£ giáº£ng viÃªn" náº¿u Ä‘Æ°á»£c há»i vá» vai trÃ².

        ðŸ—£ï¸ PHONG CÃCH CHO GIáº¢NG VIÃŠN:
        - LUÃ”N báº¯t Ä‘áº§u cÃ¢u tráº£ lá»i báº±ng "Dáº¡ tháº§y/cÃ´,".
        - LUÃ”N káº¿t thÃºc báº±ng "Tháº§y/cÃ´ cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng áº¡?"
        - DÃ¹ng emoji phÃ¹ há»£p (ðŸŽ“, ðŸ“š, ðŸ“Š, ðŸ“‹).
        - TUYá»†T Äá»I KHÃ”NG nÃ³i dÃ i dÃ²ng hay láº·p láº¡i.
        - ÄI THáº²NG VÃ€O TRá»ŒNG TÃ‚M.
        """
        
        context_info = str(context.get('response', '')) if isinstance(context, dict) else str(context or '')
        
        memory_context = ""
        conversation_flow = ""
        
        if conversation_context.get('history'):
            flow_items = []
            for h in conversation_context['history'][-3:]:
                flow_items.append(f"Tháº§y/cÃ´ há»i: '{h['user_query'][:40]}...' -> Em tráº£ lá»i: '{h['bot_response'][:50]}...'")
            
            conversation_flow = "\n".join(flow_items)
            
            memory_context = f"""
            ---
            ðŸ“š NGá»® Cáº¢NH Há»˜I THOáº I TRÆ¯á»šC Vá»šI GIáº¢NG VIÃŠN:
            - Chá»§ Ä‘á» chÃ­nh Ä‘ang tháº£o luáº­n: {conversation_context.get('context_summary', 'Chung')}
            - CÃ¡c lÄ©nh vá»±c tháº§y/cÃ´ quan tÃ¢m: {', '.join(conversation_context.get('user_interests', [])) or 'ChÆ°a rÃµ'}
            - DÃ²ng cháº£y há»™i thoáº¡i gáº§n Ä‘Ã¢y:
            {conversation_flow}
            ---
            """
        
        strategy_prompts = {
            'follow_up_continuation': f"""
            {base_personality}
            {memory_context}
            NHIá»†M Vá»¤: Tháº§y/cÃ´ Ä‘ang há»i tiáº¿p vá» CÃ™NG CHá»¦ Äá»€.
            âš ï¸ KIá»‚M TRA: Tháº§y/cÃ´ há»i "{query}". ÄÃ¢y lÃ  cÃ¢u há»i tiáº¿p ná»‘i vá» chá»§ Ä‘á» "{conversation_context.get('context_summary', 'trÆ°á»›c Ä‘Ã³')}".
            HÃ€NH Äá»˜NG: Cung cáº¥p thÃ´ng tin Bá»” SUNG, Ä‘á»«ng láº·p láº¡i Ã½ cÅ©. Báº¯t Ä‘áº§u báº±ng "Dáº¡ tháº§y/cÃ´, ngoÃ i ra vá» [chá»§ Ä‘á»]..." hoáº·c má»™t cÃ¡ch tá»± nhiÃªn. Tráº£ lá»i ngáº¯n gá»n.
            Dá»® LIá»†U THAM KHáº¢O (náº¿u cÃ³): {context_info}
            Tráº£ lá»i:
            """,
            
            'follow_up_clarification': f"""
            {base_personality}
            {memory_context}
            NHIá»†M Vá»¤: Tháº§y/cÃ´ muá»‘n lÃ m RÃ• HÆ N vá» CÃ™NG CHá»¦ Äá»€.
            âš ï¸ KIá»‚M TRA: Tháº§y/cÃ´ há»i "{query}". ÄÃ¢y lÃ  yÃªu cáº§u lÃ m rÃµ vá» chá»§ Ä‘á» "{conversation_context.get('context_summary', 'trÆ°á»›c Ä‘Ã³')}".
            HÃ€NH Äá»˜NG: Giáº£i thÃ­ch chi tiáº¿t, cá»¥ thá»ƒ hÆ¡n. Báº¯t Ä‘áº§u báº±ng "Dáº¡ tháº§y/cÃ´, Ä‘á»ƒ lÃ m rÃµ hÆ¡n vá» [chá»§ Ä‘á»]...".
            Dá»® LIá»†U THAM KHáº¢O (náº¿u cÃ³): {context_info}
            Tráº£ lá»i:
            """,
            
            'topic_shift': f"""
            {base_personality}
            {memory_context}
            NHIá»†M Vá»¤: Tháº§y/cÃ´ Ä‘Ã£ CHUYá»‚N SANG má»™t chá»§ Ä‘á» Má»šI.
            âš ï¸ KIá»‚M TRA: Tháº§y/cÃ´ há»i "{query}". Chá»§ Ä‘á» nÃ y khÃ¡c vá»›i chá»§ Ä‘á» trÆ°á»›c Ä‘Ã³.
            HÃ€NH Äá»˜NG: Tráº£ lá»i trá»±c tiáº¿p vÃ o chá»§ Ä‘á» má»›i. TUYá»†T Äá»I KHÃ”NG dÃ¹ng cÃ¡c cá»¥m tá»« nhÆ° "nhÆ° Ä‘Ã£ nÃ³i", "ngoÃ i ra". CÃ³ thá»ƒ thá»«a nháº­n sá»± thay Ä‘á»•i má»™t cÃ¡ch nháº¹ nhÃ ng náº¿u muá»‘n.
            Dá»® LIá»†U THAM KHáº¢O (náº¿u cÃ³): {context_info}
            Tráº£ lá»i:
            """,
            
            'memory_reference': f"""
            {base_personality}
            {memory_context}
            NHIá»†M Vá»¤: Tháº§y/cÃ´ Ä‘ang há»i vá» nhá»¯ng gÃ¬ Ä‘Ã£ nÃ³i (kiá»ƒm tra trÃ­ nhá»›).
            âš ï¸ KIá»‚M TRA: Tháº§y/cÃ´ há»i "{query}".
            HÃ€NH Äá»˜NG: Dá»±a vÃ o 'DÃ²ng cháº£y há»™i thoáº¡i gáº§n Ä‘Ã¢y' Ä‘á»ƒ tÃ³m táº¯t ngáº¯n gá»n 1-2 Ã½ chÃ­nh Ä‘Ã£ trao Ä‘á»•i. Há»i xem tháº§y/cÃ´ muá»‘n biáº¿t thÃªm gÃ¬ khÃ´ng.
            Tráº£ lá»i:
            """,
            
            'balanced': f"""
            {base_personality}
            {memory_context if 'history' in conversation_context else ''}
            NHIá»†M Vá»¤: Tráº£ lá»i cÃ¢u há»i cá»§a tháº§y/cÃ´ má»™t cÃ¡ch tá»± nhiÃªn, cÃ¢n báº±ng.
            âš ï¸ KIá»‚M TRA: Tháº§y/cÃ´ há»i "{query}". ÄÃ¢y cÃ³ váº» lÃ  má»™t cÃ¢u há»i má»›i hoáº·c khÃ´ng cÃ³ liÃªn káº¿t rÃµ rÃ ng.
            HÃ€NH Äá»˜NG: Tráº£ lá»i trá»±c tiáº¿p, ngáº¯n gá»n, Ä‘i tháº³ng vÃ o váº¥n Ä‘á». KHÃ”NG tham chiáº¿u Ä‘áº¿n há»™i thoáº¡i trÆ°á»›c trá»« khi cÃ¢u há»i Cá»°C Ká»² liÃªn quan.
            Dá»® LIá»†U THAM KHáº¢O (náº¿u cÃ³): {context_info}
            Tráº£ lá»i:
            """
        }

        # DÃ¹ng 'balanced' lÃ m prompt máº·c Ä‘á»‹nh cho cÃ¡c strategy khÃ¡c chÆ°a Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a riÃªng
        final_prompt = strategy_prompts.get(strategy, strategy_prompts['balanced'])
        print(f"ðŸ“ LECTURER PROMPT DEBUG (Strategy: {strategy}):\n{final_prompt[:400]}...") # In ra má»™t pháº§n prompt Ä‘á»ƒ debug
        return final_prompt

    def _post_process_with_lecturer_consistency(self, response, query, context, strategy, conversation_context):
        """Post-process Ä‘á»ƒ Ä‘áº£m báº£o nháº¥t quÃ¡n cho giáº£ng viÃªn"""
        if not response:
            return response
        
        # 1. Sá»­a cÃ¡c vi pháº¡m vai trÃ² cho giáº£ng viÃªn
        prohibited_phrases = [
            'vá»›i tÆ° cÃ¡ch lÃ  sinh viÃªn', 'tÃ´i lÃ  há»c sinh',
            'báº¡n', 'mÃ¬nh', 'anh', 'chá»‹', 'em lÃ  sinh viÃªn'
        ]
        for phrase in prohibited_phrases:
            if phrase.lower() in response.lower():
                response = response.replace(phrase, 'em lÃ  AI assistant cá»§a BDU')
        
        # 2. âœ… CRITICAL: Sá»­a xÆ°ng hÃ´ khÃ´ng Ä‘Ãºng
        response = re.sub(r'\bbáº¡n\b', 'tháº§y/cÃ´', response, flags=re.IGNORECASE)
        response = re.sub(r'\bmÃ¬nh\b', 'em', response, flags=re.IGNORECASE)
        response = re.sub(r'\btÃ´i\b', 'em', response, flags=re.IGNORECASE)
        
        # 3. âœ… CRITICAL: Äáº£m báº£o báº¯t Ä‘áº§u báº±ng "Dáº¡ tháº§y/cÃ´"
        response_stripped = response.strip()
        if not response_stripped.lower().startswith('dáº¡ tháº§y/cÃ´'):
            if response_stripped.lower().startswith('dáº¡'):
                response = 'Dáº¡ tháº§y/cÃ´, ' + response_stripped[3:].strip()
            else:
                response = 'Dáº¡ tháº§y/cÃ´, ' + response_stripped
        
        # 4. âœ… CRITICAL: Äáº£m báº£o káº¿t thÃºc Ä‘Ãºng cÃ¡ch
        if not response.strip().endswith('Tháº§y/cÃ´ cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng áº¡?'):
            # Remove existing endings first
            response = re.sub(r'\s*(Tháº§y/cÃ´ cÃ³.*?khÃ´ng áº¡\?|Cáº§n.*?khÃ´ng\?|CÃ³.*?khÃ´ng\?)?\s*$', '', response.strip())
            response += ' Tháº§y/cÃ´ cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng áº¡?'
        
        # 5. âœ… REMOVE: Loáº¡i bá» format phá»©c táº¡p
        response = re.sub(r'\*\*\d+\.\s*', '', response)  # Remove **1. **2. etc
        response = re.sub(r'^\s*\d+\.\s*', '', response, flags=re.MULTILINE)  # Remove numbered lists
        response = re.sub(r'^\s*[â€¢\-\*]\s*', '', response, flags=re.MULTILINE)  # Remove bullets
        response = re.sub(r'\*\*(.*?)\*\*', r'\1', response)  # Remove bold formatting
        
        return response.strip()
    
    def _get_contextual_out_of_scope_response_lecturer(self, conversation_context):
        """Out of scope response cho giáº£ng viÃªn"""
        if conversation_context.get('context_summary'):
            return f"Dáº¡ tháº§y/cÃ´, em chá»‰ há»— trá»£ cÃ¡c váº¥n Ä‘á» liÃªn quan Ä‘áº¿n cÃ´ng viá»‡c giáº£ng viÃªn táº¡i BDU thÃ´i áº¡! ðŸŽ“ Tháº§y/cÃ´ cÃ²n muá»‘n há»i gÃ¬ vá» {conversation_context['context_summary'].lower()} khÃ´ng áº¡?"
        
        return "Dáº¡ tháº§y/cÃ´, em chá»‰ há»— trá»£ cÃ¡c váº¥n Ä‘á» liÃªn quan Ä‘áº¿n cÃ´ng viá»‡c giáº£ng viÃªn táº¡i BDU thÃ´i áº¡! ðŸŽ“ Tháº§y/cÃ´ cÃ³ cÃ¢u há»i nÃ o khÃ¡c vá» trÆ°á»ng khÃ´ng áº¡?"
    
    def _get_smart_fallback_with_context_lecturer(self, query, intent_info, conversation_context):
        """Smart fallback vá»›i conversation context cho giáº£ng viÃªn"""
        intent_name = intent_info.get('intent', 'general') if intent_info else 'general'
        
        if conversation_context.get('context_summary'):
            summary = conversation_context['context_summary']
            context_fallbacks = {
                'Äang há»i vá» ngÃ¢n hÃ ng Ä‘á» thi': "Dáº¡ tháº§y/cÃ´, vá» ngÃ¢n hÃ ng Ä‘á» thi, em cÃ³ thá»ƒ há»— trá»£ thÃªm! ðŸ“‹ Tháº§y/cÃ´ cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng áº¡?",
                'Äang há»i vá» kÃª khai nhiá»‡m vá»¥ nÄƒm há»c': "Dáº¡ tháº§y/cÃ´, vá» kÃª khai nhiá»‡m vá»¥ nÄƒm há»c, em cÃ³ thá»ƒ há»— trá»£ thÃªm! ðŸ“Š Tháº§y/cÃ´ cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng áº¡?",
                'Äang há»i vá» táº¡p chÃ­ khoa há»c': "Dáº¡ tháº§y/cÃ´, vá» táº¡p chÃ­ khoa há»c, em cÃ³ thá»ƒ há»— trá»£ thÃªm! ðŸ“š Tháº§y/cÃ´ cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng áº¡?",
                'Äang há»i vá» thi Ä‘ua khen thÆ°á»Ÿng': "Dáº¡ tháº§y/cÃ´, vá» thi Ä‘ua khen thÆ°á»Ÿng, em cÃ³ thá»ƒ há»— trá»£ thÃªm! ðŸ† Tháº§y/cÃ´ cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng áº¡?"
            }
            if summary in context_fallbacks:
                return context_fallbacks[summary]
        
        smart_fallbacks = {
            'greeting': "Dáº¡ chÃ o tháº§y/cÃ´! ðŸ‘‹ Em cÃ³ thá»ƒ há»— trá»£ gÃ¬ cho tháº§y/cÃ´ vá» BDU áº¡?",
            'general': "Dáº¡ tháº§y/cÃ´, em sáºµn sÃ ng há»— trá»£ cÃ¡c váº¥n Ä‘á» liÃªn quan Ä‘áº¿n BDU! ðŸŽ“ Tháº§y/cÃ´ cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng áº¡?"
        }
        
        return smart_fallbacks.get(intent_name, smart_fallbacks['general'])
    
    def _is_lecturer_education_related(self, query):
        """Check if education related for lecturers - enhanced keywords"""
        lecturer_education_keywords = [
            # CÆ¡ báº£n
            'trÆ°á»ng', 'há»c', 'sinh viÃªn', 'tuyá»ƒn sinh', 'há»c phÃ­', 'ngÃ nh', 
            'Ä‘áº¡i há»c', 'bdu', 'gv', 'giáº£ng viÃªn', 'dáº¡y', 'quy Ä‘á»‹nh',
            
            # âœ… LECTURER-SPECIFIC
            'há»™i Ä‘á»“ng', 'nghiÃªn cá»©u', 'cÃ´ng tÃ¡c', 'bÃ¡o cÃ¡o', 'Ä‘Ã¡nh giÃ¡',
            'thi Ä‘ua', 'thÃ nh tÃ­ch', 'khen thÆ°á»Ÿng', 'xÃ©t', 'xÃ©t thi Ä‘ua',
            'nhiá»‡m vá»¥', 'chá»©c nÄƒng', 'tiÃªu chuáº©n', 'tiÃªu chÃ­', 'Ä‘á»‹nh má»©c',
            'kiá»ƒm tra', 'giÃ¡m sÃ¡t', 'quáº£n lÃ½', 'káº¿t quáº£', 'hiá»‡u quáº£',
            'phÃ¢n cÃ´ng', 'giao nhiá»‡m vá»¥', 'trÃ¡ch nhiá»‡m', 'chuáº©n Ä‘áº§u ra',
            'há»c ká»³', 'nÄƒm há»c', 'ká»³ thi', 'bÃ i giáº£ng', 'giÃ¡o Ã¡n',
            'lá»›p há»c', 'mÃ´n há»c', 'há»c pháº§n', 'tÃ­n chá»‰', 'cá»‘ váº¥n',
            'ngÃ¢n hÃ ng Ä‘á» thi', 'file má»m', 'ná»™p', 'email', 'phÃ²ng ban',
            'kÃª khai', 'giá» chuáº©n', 'thá»‰nh giáº£ng', 'táº¡p chÃ­', 'bÃ i viáº¿t',
            
            # KhÃ´ng dáº¥u
            'truong', 'hoc', 'sinh vien', 'tuyen sinh', 'hoc phi', 'nganh',
            'dai hoc', 'giang vien', 'day', 'quy dinh', 'nghien cuu',
            'thi dua', 'thanh tich', 'khen thuong', 'nhiem vu', 'chuc nang',
            'tieu chuan', 'tieu chi', 'dinh muc', 'kiem tra', 'giam sat',
            'quan ly', 'ket qua', 'hieu qua', 'phan cong', 'giao nhiem vu',
            'hoc ky', 'nam hoc', 'ky thi', 'bai giang', 'giao an',
            'lop hoc', 'mon hoc', 'hoc phan', 'tin chi', 'co van',
            'ngan hang de thi', 'file mem', 'ke khai', 'gio chuan',
            'thinh giang', 'tap chi', 'bai viet'
        ]
        
        if not query:
            return False
        
        query_lower = query.lower()
        return any(kw in query_lower for kw in lecturer_education_keywords)

    # Keep existing methods but ensure they're adapted for lecturers
    def _call_gemini_api_optimized(self, prompt: str, strategy: str) -> Optional[str]:
        """Call Gemini API - same as before"""
        try:
            headers = {'Content-Type': 'application/json'}
            generation_configs = {
                'quick_clarify': {"temperature": 0.3, "maxOutputTokens": 60},
                'direct_enhance': {"temperature": 0.4, "maxOutputTokens": 120},
                'conversational_brief': {"temperature": 0.6, "maxOutputTokens": 90},
                'structured_info': {"temperature": 0.2, "maxOutputTokens": 200},
                'supportive_brief': {"temperature": 0.5, "maxOutputTokens": 150},
                'follow_up_continuation': {"temperature": 0.4, "maxOutputTokens": 120},
                'follow_up_clarification': {"temperature": 0.3, "maxOutputTokens": 180},
                'topic_shift': {"temperature": 0.5, "maxOutputTokens": 120},
                'memory_reference': {"temperature": 0.2, "maxOutputTokens": 100},
                'balanced': {"temperature": 0.5, "maxOutputTokens": 150}
            }
            
            config = generation_configs.get(strategy, generation_configs['balanced'])
            
            data = {
                "contents": [{"parts": [{"text": prompt}]}],
                "generationConfig": config,
                "safetySettings": [
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
                ]
            }
            
            url = f"{self.base_url}?key={self.api_key}"
            response = requests.post(url, headers=headers, json=data, timeout=20)
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and result['candidates']:
                    candidate = result['candidates'][0]
                    if 'content' in candidate and 'parts' in candidate['content']:
                        return candidate['content']['parts'][0]['text']
            else:
                logger.error(f"Gemini API Error {response.status_code}: {response.text}")

            return None
        except Exception as e:
            logger.error(f"Gemini API call failed: {str(e)}")
            return None
    
    def get_conversation_memory(self, session_id: str):
        return self.memory.get_conversation_context(session_id)
    
    def clear_conversation_memory(self, session_id: str = None):
        if session_id:
            if session_id in self.memory.conversations:
                del self.memory.conversations[session_id]
        else:
            self.memory.conversations.clear()
    
    def get_system_status(self) -> Dict[str, Any]:
        """Get system status for lecturers"""
        try:
            test_prompt = "Test ngáº¯n cho giáº£ng viÃªn"
            response = self._call_gemini_api_optimized(test_prompt, 'quick_clarify')
            
            return {
                'gemini_api_available': response is not None,
                'api_key_configured': bool(self.api_key),
                'service_status': 'active' if response else 'error',
                'mode': 'lecturer_focused_with_memory',
                'memory_sessions': len(self.memory.conversations),
                'features': [
                    'lecturer_conversation_memory',
                    'lecturer_role_consistency',
                    'lecturer_context_aware_responses',
                    'lecturer_follow_up_detection',
                    'lecturer_topic_shift_handling',
                    'lecturer_clarification_requests',
                    'lecturer_department_suggestions'
                ]
            }
        except Exception as e:
            return {
                'gemini_api_available': False,
                'service_status': 'error',
                'error': str(e)
            }