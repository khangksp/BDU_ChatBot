import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import time
import pickle
import os
import re
from django.conf import settings
from knowledge.models import KnowledgeBase
import logging
from .phobert_service import PhoBERTIntentClassifier
from .gemini_service import GeminiResponseGenerator
import pandas as pd

logger = logging.getLogger(__name__)

class LecturerDecisionEngine:
    """
    Enhanced Decision Engine specifically for BDU Lecturers
    """
    
    def __init__(self):
        # ‚úÖ ENHANCED: Confidence thresholds for lecturers
        self.confidence_thresholds = {
            'high_trust': 0.7,      # Very specific lecturer info - use direct
            'medium_trust': 0.5,    # Related info - enhance with context
            'low_trust': 0.25,      # Uncertain - ask for clarification
            'no_trust': 0.1         # No match - say don't know
        }
        
        # ‚úÖ EXPANDED: Education keywords CHO GI·∫¢NG VI√äN BDU
        self.education_keywords = [
            # T·ª´ kh√≥a c∆° b·∫£n v·ªÅ gi√°o d·ª•c
            'h·ªçc', 'tr∆∞·ªùng', 'sinh vi√™n', 'tuy·ªÉn sinh', 'h·ªçc ph√≠', 'ng√†nh', 
            'ƒë·∫°i h·ªçc', 'bdu', 'gv', 'gi·∫£ng vi√™n', 'd·∫°y', 'quy ƒë·ªãnh', 'khoa',
            'ch∆∞∆°ng tr√¨nh', 'ƒë√†o t·∫°o', 'l·ªãch', 'th·ªùi kh√≥a bi·ªÉu', 'ph√≤ng', 'l·ªõp',
            
            # ‚úÖ CRITICAL: Th√™m t·ª´ kh√≥a cho SINH VI√äN (v√¨ gi·∫£ng vi√™n c≈©ng h·ªèi v·ªÅ sinh vi√™n)
            'l·ªá ph√≠', 'ph√≠', 'ti·ªÅn', 'b·∫±ng', 'vƒÉn b·∫±ng', 't·ªët nghi·ªáp', 'nh·∫≠n b·∫±ng',
            'chuy·ªÉn kho·∫£n', 'thanh to√°n', 'n·ªôp ti·ªÅn', 'ƒë√≥ng ph√≠', 'thu ng√¢n',
            'k·∫ø to√°n', 't√†i ch√≠nh', 'ƒëi·ªÉm', 'transcript', 'b·∫£ng ƒëi·ªÉm',
            'th·ªß t·ª•c', 'gi·∫•y t·ªù', 'h·ªì s∆°', 'ƒëƒÉng k√Ω', 'xin c·∫•p',
            'le phi', 'phi', 'tien', 'bang', 'van bang', 'tot nghiep', 'nhan bang',
            'chuyen khoan', 'thanh toan', 'nop tien', 'dong phi', 'thu ngan',
            'ke toan', 'tai chinh', 'diem', 'bang diem',
            'thu tuc', 'giay to', 'ho so', 'dang ky', 'xin cap',
            
            # ‚úÖ TH√äM: T·ª´ kh√≥a QUAN TR·ªåNG cho GI·∫¢NG VI√äN (extracted from QA.csv analysis)
            'h·ªôi ƒë·ªìng', 'nghi√™n c·ª©u', 'c√¥ng t√°c', 'b√°o c√°o', 'ƒë√°nh gi√°',
            'thi ƒëua', 'th√†nh t√≠ch', 'khen th∆∞·ªüng', 'x√©t', 'x√©t thi ƒëua',
            'c√° nh√¢n', 't·∫≠p th·ªÉ', 'ho√†n th√†nh', 'nhi·ªám v·ª•', 'ch·ª©c nƒÉng',
            'ti√™u chu·∫©n', 'ti√™u ch√≠', 'ƒë·ªãnh m·ª©c', 'ch·∫•t l∆∞·ª£ng',
            'ki·ªÉm tra', 'gi√°m s√°t', 'qu·∫£n l√Ω', 'v·∫≠n h√†nh',
            'k·∫øt qu·∫£', 'hi·ªáu qu·∫£', 'th·ª±c hi·ªán', 'ho·∫°t ƒë·ªông',
            'ph√¢n c√¥ng', 'giao nhi·ªám v·ª•', 'tr√°ch nhi·ªám',
            'chu·∫©n ƒë·∫ßu ra', 'm·ª•c ti√™u', 'ch·ªâ ti√™u', 'k·∫ø ho·∫°ch',
            'h·ªçc k·ª≥', 'nƒÉm h·ªçc', 'k·ª≥ thi', 'thi c·ª≠', 'ƒëi·ªÉm s·ªë',
            'b√†i gi·∫£ng', 'gi√°o √°n', 't√†i li·ªáu', 'gi√°o tr√¨nh',
            'l·ªõp h·ªçc', 'm√¥n h·ªçc', 'h·ªçc ph·∫ßn', 't√≠n ch·ªâ',
            'c·ªë v·∫•n', 'h∆∞·ªõng d·∫´n', 't∆∞ v·∫•n', 'h·ªó tr·ª£',
            '·∫£nh h∆∞·ªüng', 'm·∫•t l√≤ng', 'xu·∫•t s·∫Øc', 'ƒë·ªìng nghi·ªáp',
            
            # ‚úÖ CRITICAL: T·ª´ kh√≥a c·ª• th·ªÉ t·ª´ QA.csv cho gi·∫£ng vi√™n
            'ng√¢n h√†ng ƒë·ªÅ thi', 'file m·ªÅm', 'b√°o c√°o', 'n·ªôp', 'h·∫°n cu·ªëi',
            'email', 'ph√≤ng ban', 'ƒë∆°n v·ªã', 'khoa', 'b·ªô m√¥n',
            'k√™ khai', 'nhi·ªám v·ª• nƒÉm h·ªçc', 'gi·ªù chu·∫©n', 'th·ªânh gi·∫£ng', 'c∆° h·ªØu',
            't·∫°p ch√≠', 'khoa h·ªçc c√¥ng ngh·ªá', 'b√†i vi·∫øt', 'nghi√™n c·ª©u',
            'l·ªÖ khen th∆∞·ªüng', 'b·∫±ng khen', 'danh hi·ªáu', 'c√¥ng nh·∫≠n',
            'ph√≤ng ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng', 'kh·∫£o th√≠', 'ph√≤ng t·ªï ch·ª©c c√°n b·ªô',
            'quy·∫øt ƒë·ªãnh', 'th√¥ng b√°o', 'vƒÉn b·∫£n', 'tri·ªÉn khai',
            'c·∫≠p nh·∫≠t', 'd·ªØ li·ªáu', 'ph·∫ßn m·ªÅm', 'qu·∫£n l√Ω ƒë√†o t·∫°o',
            'ho·∫°t ƒë·ªông gi·∫£ng d·∫°y', 'c√¥ng t√°c gi·∫£ng d·∫°y', 'ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng',
            
            # T·ª´ kh√≥a kh√¥ng d·∫•u (QUAN TR·ªåNG cho search)
            'hoc', 'truong', 'sinh vien', 'tuyen sinh', 'hoc phi', 'nganh',
            'dai hoc', 'giang vien', 'day', 'quy dinh', 'chuong trinh', 'dao tao',
            'thi dua', 'thanh tich', 'khen thuong', 'xet', 'xet thi dua',
            'ca nhan', 'tap the', 'hoan thanh', 'nhiem vu', 'chuc nang',
            'tieu chuan', 'tieu chi', 'dinh muc', 'chat luong',
            'kiem tra', 'giam sat', 'quan ly', 'van hanh',
            'ket qua', 'hieu qua', 'thuc hien', 'hoat dong',
            'phan cong', 'giao nhiem vu', 'trach nhiem',
            'chuan dau ra', 'muc tieu', 'chi tieu', 'ke hoach',
            'hoc ky', 'nam hoc', 'ky thi', 'thi cu', 'diem so',
            'bai giang', 'giao an', 'tai lieu', 'giao trinh',
            'lop hoc', 'mon hoc', 'hoc phan', 'tin chi',
            'co van', 'huong dan', 'tu van', 'ho tro',
            'anh huong', 'mat long', 'xuat sac', 'dong nghiep',
            'ngan hang de thi', 'file mem', 'bao cao', 'nop', 'han cuoi',
            'ke khai', 'nhiem vu nam hoc', 'gio chuan', 'thinh giang', 'co huu',
            'tap chi', 'khoa hoc cong nghe', 'bai viet', 'nghien cuu',
            'le khen thuong', 'bang khen', 'danh hieu', 'cong nhan'
        ]
        
        # ‚úÖ TH√äM: Gi·∫£ng vi√™n specific keywords
        self.lecturer_keywords = [
            'gi·∫£ng vi√™n', 'gv', 'th·∫ßy', 'c√¥', 'ph·ª• tr√°ch', 'gi·∫£ng d·∫°y',
            'nghi√™n c·ª©u', 'h·ªôi ƒë·ªìng', 'khoa', 'b·ªô m√¥n', 'chuy√™n ng√†nh',
            'giang vien', 'phu trach', 'giang day', 'nghien cuu', 'chuyen nganh'
        ]
        
        # ‚úÖ TH√äM: Keywords require clarification (c√¢u h·ªèi m∆° h·ªì)
        self.vague_keywords = [
            'l√†m sao', 'nh∆∞ th·∫ø n√†o', 'c√°ch n√†o', 'th·ªß t·ª•c', 'quy tr√¨nh',
            'th√¥ng tin', 'chi ti·∫øt', 'h∆∞·ªõng d·∫´n', 'gi√∫p ƒë·ª°', 'h·ªó tr·ª£',
            'g√¨', 'n√†o', 'khi n√†o', '·ªü ƒë√¢u', 'ai', 'sao', 'c√≥ ph·∫£i',
            'lam sao', 'nhu the nao', 'cach nao', 'thu tuc', 'quy trinh',
            'thong tin', 'chi tiet', 'huong dan', 'giup do', 'ho tro'
        ]
        
        logger.info("‚úÖ LecturerDecisionEngine initialized for LECTURERS with expanded keywords")
    
    def is_education_related(self, query):
        """Enhanced education detection for lecturers with memory context"""
        if not query:
            return False
        
        query_lower = query.lower()
        
        # ‚úÖ CRITICAL: T√¨m ki·∫øm b·∫•t k·ª≥ t·ª´ kh√≥a n√†o c√≥ trong c√¢u h·ªèi
        found_keywords = []
        for kw in self.education_keywords:
            if kw in query_lower:
                found_keywords.append(kw)
        
        # Count education keywords
        education_count = len(found_keywords)
        lecturer_count = sum(1 for kw in self.lecturer_keywords if kw in query_lower)
        
        # ‚úÖ LOOSENED: Ch·ªâ c·∫ßn 1 keyword education ho·∫∑c lecturer
        is_education = education_count >= 1 or lecturer_count >= 1
        
        # ‚úÖ SPECIAL: N·∫øu kh√¥ng t√¨m th·∫•y keyword, ki·ªÉm tra c√°c pattern ph·ªï bi·∫øn
        if not is_education:
            # Ki·ªÉm tra c√°c pattern v·ªÅ gi√°o d·ª•c
            education_patterns = [
                r'ph√≠.*(?:h·ªçc|t·ªët nghi·ªáp|nh·∫≠n|c·∫•p)',
                r'(?:h·ªçc|ph√≠|ti·ªÅn).*(?:ph√≠|h·ªçc|c·∫•p|nh·∫≠n)',
                r'(?:b·∫±ng|vƒÉn b·∫±ng|t·ªët nghi·ªáp)',
                r'(?:th·ªß t·ª•c|quy tr√¨nh|c√°ch th·ª©c)',
                r'(?:bdu|ƒë·∫°i h·ªçc|tr∆∞·ªùng)',
                r'(?:sinh vi√™n|h·ªçc sinh)',
                r'(?:gi·∫£ng vi√™n|th·∫ßy|c√¥|gv)'
            ]
            
            for pattern in education_patterns:
                if re.search(pattern, query_lower):
                    is_education = True
                    found_keywords.append(f"pattern:{pattern}")
                    break
        
        logger.info(f"üéì Education check: '{query}' -> keywords:{found_keywords} -> {is_education}")
        return is_education
    
    def needs_clarification(self, query, confidence):
        """Check if query needs clarification"""
        if not query:
            return False
            
        query_lower = query.lower()
        
        # Check for vague questions
        vague_count = sum(1 for kw in self.vague_keywords if kw in query_lower)
        word_count = len(query.split())
        
        # Very short + vague OR low confidence
        needs_clarification = (
            (vague_count >= 2 and word_count <= 5) or 
            (confidence < self.confidence_thresholds['low_trust'] and vague_count >= 1)
        )
        
        logger.info(f"‚ùì Clarification check: vague:{vague_count}, words:{word_count}, conf:{confidence:.3f} -> {needs_clarification}")
        return needs_clarification
    
    def categorize_confidence(self, similarity_score):
        """Categorize confidence level"""
        if similarity_score >= self.confidence_thresholds['high_trust']:
            return 'high_trust'
        elif similarity_score >= self.confidence_thresholds['medium_trust']:
            return 'medium_trust'  
        elif similarity_score >= self.confidence_thresholds['low_trust']:
            return 'low_trust'
        else:
            return 'no_trust'
    
    def make_decision(self, query, retrieval_result, intent_result, session_memory=None):
        """Enhanced decision making for lecturers with memory context"""
        
        # Step 1: Check conversation context first
        context_override = False
        if session_memory and len(session_memory) > 0:
            # Ki·ªÉm tra 3 c√¢u h·ªèi g·∫ßn nh·∫•t c√≥ ph·∫£i v·ªÅ education kh√¥ng
            recent_queries = [item.get('query', '') for item in session_memory[-3:]]
            recent_education_queries = [q for q in recent_queries if self.is_education_related(q)]
            
            # N·∫øu c√≥ √≠t nh·∫•t 1 c√¢u g·∫ßn ƒë√¢y v·ªÅ education -> cho ph√©p c√¢u hi·ªán t·∫°i
            if len(recent_education_queries) >= 1:
                context_override = True
                logger.info(f"üß† MEMORY OVERRIDE: Recent education context detected - allowing current query")
        
        # Step 2: Check if education-related
        is_education = self.is_education_related(query) or context_override
        
        if not is_education:
            return 'reject_non_education', None, False
        
        # Step 3: Get confidence level
        similarity = retrieval_result.get('confidence', 0)
        confidence_level = self.categorize_confidence(similarity)
        
        # Step 4: Check if needs clarification
        needs_clarification = self.needs_clarification(query, similarity)
        
        logger.info(f"ü§ñ Decision inputs: education={is_education}, context_override={context_override}, similarity={similarity:.3f}, level={confidence_level}, clarify={needs_clarification}")
        
        # Step 5: Make decision
        if needs_clarification:
            return 'ask_clarification', {
                'query': query,
                'confidence': similarity,
                'instruction': 'clarification_needed',
                'message': 'Question is too vague, need clarification'
            }, True
        
        if confidence_level == 'high_trust':
            decision = 'use_db_direct'
            context = {
                'instruction': 'direct_answer_lecturer',
                'db_answer': retrieval_result.get('response', ''),
                'confidence': similarity,
                'message': 'High confidence - use database answer directly'
            }
            
        elif confidence_level == 'medium_trust':
            decision = 'enhance_db_answer'
            context = {
                'instruction': 'enhance_answer_lecturer',
                'db_answer': retrieval_result.get('response', ''),
                'confidence': similarity,
                'message': 'Medium confidence - enhance database answer'
            }
            
        elif confidence_level == 'low_trust':
            decision = 'ask_clarification'
            context = {
                'instruction': 'clarification_needed',
                'db_answer': retrieval_result.get('response', ''),
                'confidence': similarity,
                'message': 'Low confidence - ask for clarification'
            }
            
        else:  # no_trust
            decision = 'say_dont_know'
            context = {
                'instruction': 'dont_know_lecturer',
                'confidence': similarity,
                'message': 'No relevant information - say dont know'
            }
        
        logger.info(f"üéØ Decision made: {decision}")
        return decision, context, True


class HybridChatbotAI:
    """
    Enhanced Hybrid Chatbot specifically for BDU Lecturers
    """
    
    def __init__(self):
        # Initialize components with lecturer-specific enhancements
        self.sbert_retriever = ChatbotAI()
        self.intent_classifier = PhoBERTIntentClassifier()
        self.response_generator = GeminiResponseGenerator()  # Now uses enhanced version
        self.decision_engine = LecturerDecisionEngine()  # New lecturer-specific engine
        
        # Enhanced conversation memory for lecturers
        self.conversation_memory = {}
        
        logger.info("üöÄ HybridChatbotAI initialized specifically for BDU Lecturers")
    
    @property
    def model(self):
        return self.sbert_retriever.model
    
    @property
    def index(self):
        return self.sbert_retriever.index
    
    @property
    def knowledge_data(self):
        return self.sbert_retriever.knowledge_data
    
    def get_system_status(self):
        """Get system status for lecturers"""
        gemini_status = self.response_generator.get_system_status()
        
        return {
            'sbert_model': bool(self.sbert_retriever.model),
            'faiss_index': bool(self.sbert_retriever.index),
            'phobert_available': not self.intent_classifier.fallback_mode,
            'gemini_available': gemini_status.get('gemini_api_available', False),
            'knowledge_entries': len(self.sbert_retriever.knowledge_data),
            'mode': 'lecturer_focused_hybrid_with_clarification',
            'memory_sessions': gemini_status.get('memory_sessions', 0),
            'confidence_thresholds': self.decision_engine.confidence_thresholds,
            'lecturer_features': [
                'lecturer_keyword_detection',
                'clarification_requests', 
                'department_suggestions',
                'formal_addressing',
                'concise_responses',
                'no_fabrication_policy'
            ],
            'gemini_status': gemini_status
        }
    
    def process_query(self, query, session_id=None):
        """
        Main query processing specifically optimized for lecturers
        """
        start_time = time.time()
        
        logger.info(f"üë®‚Äçüè´ Processing lecturer query: '{query}' (session: {session_id})")
        
        try:
            # Step 1: Clean and validate input
            query = self._clean_query(query)
            if not query or len(query.strip()) < 2:
                return self._get_empty_query_response_lecturer()
            
            # Step 2: Get intent and entities
            intent_result = self.intent_classifier.classify_intent(query)
            entities = self.intent_classifier.extract_entities(query)
            
            # Step 3: Search knowledge base
            retrieval_result = self.sbert_retriever.generate_response(query)
            
            logger.info(f"üîç Retrieval result: confidence={retrieval_result.get('confidence', 0):.3f}")
            
            # Step 4: Make lecturer-specific decision WITH MEMORY CONTEXT
            session_memory = self.get_conversation_context(session_id) if session_id else None
            decision_type, gemini_context, should_respond = self.decision_engine.make_decision(
                query, retrieval_result, intent_result, session_memory
            )
            
            # Step 5: Execute decision
            if not should_respond:
                response_text = "D·∫° th·∫ßy/c√¥, em ch·ªâ h·ªó tr·ª£ c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn c√¥ng vi·ªác gi·∫£ng vi√™n t·∫°i BDU th√¥i ·∫°. üéì Th·∫ßy/c√¥ c√≥ c√¢u h·ªèi n√†o kh√°c v·ªÅ tr∆∞·ªùng kh√¥ng ·∫°?"
                method = 'rejected_non_education'
            else:
                response_text = self._execute_lecturer_decision(
                    decision_type, query, gemini_context, intent_result, entities, session_id
                )
                method = decision_type
            
            # Step 6: Update memory WITH MORE DETAILS
            if session_id and should_respond:
                self._update_memory(session_id, query, intent_result, retrieval_result.get('confidence', 0), decision_type, should_respond)
            
            processing_time = time.time() - start_time
            
            return {
                'response': response_text,
                'confidence': retrieval_result.get('confidence', 0),
                'method': method,
                'decision_type': decision_type,
                'intent': intent_result,
                'sources': retrieval_result.get('sources', []),
                'entities': entities,
                'processing_time': processing_time,
                'is_education': gemini_context is not None,
                'lecturer_optimized': True
            }
            
        except Exception as e:
            logger.error(f"‚ùå Processing error: {str(e)}")
            return {
                'response': "D·∫° th·∫ßy/c√¥, em g·∫∑p kh√≥ khƒÉn k·ªπ thu·∫≠t. Th·∫ßy/c√¥ c√≥ th·ªÉ li√™n h·ªá b·ªô ph·∫≠n IT qua email it@bdu.edu.vn ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ ·∫°. üéì",
                'confidence': 0.0,
                'method': 'error_fallback',
                'processing_time': time.time() - start_time,
                'error': str(e)
            }
    
    def _execute_lecturer_decision(self, decision_type, query, gemini_context, intent_result, entities, session_id):
        """Execute lecturer-specific decisions"""
        
        logger.info(f"üéØ Executing lecturer decision: {decision_type}")
        
        if decision_type == 'use_db_direct':
            # High confidence -> Use database answer directly with lecturer formatting
            response = self.response_generator.generate_response(
                query=query,
                context=gemini_context,
                intent_info=intent_result,
                entities=entities,
                session_id=session_id
            )
            return response.get('response', f"D·∫° th·∫ßy/c√¥, {gemini_context['db_answer']} üéì Th·∫ßy/c√¥ c√≥ c·∫ßn h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?")
            
        elif decision_type == 'enhance_db_answer':
            # Medium confidence -> Enhance database answer
            response = self.response_generator.generate_response(
                query=query,
                context=gemini_context,
                intent_info=intent_result,
                entities=entities,
                session_id=session_id
            )
            return response.get('response', f"D·∫° th·∫ßy/c√¥, {gemini_context['db_answer']} üéì Th·∫ßy/c√¥ c√≥ c·∫ßn h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?")
            
        elif decision_type == 'ask_clarification':
            # Need clarification -> Generate clarification request
            response = self.response_generator.generate_response(
                query=query,
                context=gemini_context,
                intent_info=intent_result,
                entities=entities,
                session_id=session_id
            )
            return response.get('response', self._get_default_clarification_request(query))
            
        elif decision_type == 'say_dont_know':
            # No relevant info -> Generate don't know response with department suggestion
            response = self.response_generator.generate_response(
                query=query,
                context=gemini_context,
                intent_info=intent_result,
                entities=entities,
                session_id=session_id
            )
            return response.get('response', self._get_default_dont_know_response(query))
            
        else:
            logger.warning(f"‚ö†Ô∏è Unknown decision type: {decision_type}")
            return "D·∫° th·∫ßy/c√¥, em g·∫∑p kh√≥ khƒÉn trong vi·ªác x·ª≠ l√Ω c√¢u h·ªèi. Th·∫ßy/c√¥ c√≥ c·∫ßn h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°? üéì"
    
    def _get_default_clarification_request(self, query):
        """Default clarification request if Gemini fails"""
        # Extract key topic for targeted clarification
        query_words = query.lower().split()
        
        topic_keywords = {
            'ng√¢n h√†ng ƒë·ªÅ thi': ['ng√¢n h√†ng', 'ƒë·ªÅ thi', 'ƒë·ªÅ'],
            'k√™ khai nhi·ªám v·ª•': ['k√™ khai', 'nhi·ªám v·ª•'],
            't·∫°p ch√≠': ['t·∫°p ch√≠', 'b√†i vi·∫øt'],
            'thi ƒëua khen th∆∞·ªüng': ['thi ƒëua', 'khen th∆∞·ªüng'],
            'b√°o c√°o': ['b√°o c√°o', 'n·ªôp'],
            'l·ªãch gi·∫£ng d·∫°y': ['l·ªãch', 'gi·∫£ng d·∫°y']
        }
        
        for topic, keywords in topic_keywords.items():
            if any(kw in query_words for kw in keywords):
                return f"D·∫° th·∫ßy/c√¥, ƒë·ªÉ em h·ªó tr·ª£ ch√≠nh x√°c v·ªÅ {topic}, th·∫ßy/c√¥ c√≥ th·ªÉ n√≥i r√µ h∆°n v·ªÅ n·ªôi dung c·ª• th·ªÉ c·∫ßn h·ªó tr·ª£ kh√¥ng ·∫°? üéì"
        
        return "D·∫° th·∫ßy/c√¥, ƒë·ªÉ em h·ªó tr·ª£ ch√≠nh x√°c nh·∫•t, th·∫ßy/c√¥ c√≥ th·ªÉ n√≥i r√µ h∆°n v·ªÅ v·∫•n ƒë·ªÅ c·∫ßn h·ªó tr·ª£ kh√¥ng ·∫°? üéì"
    
    def _get_default_dont_know_response(self, query):
        """Default don't know response with department suggestion"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ['ng√¢n h√†ng ƒë·ªÅ', 'ƒë·ªÅ thi', 'kh·∫£o th√≠']):
            return "D·∫° th·∫ßy/c√¥, em ch∆∞a c√≥ th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y. Th·∫ßy/c√¥ c√≥ th·ªÉ li√™n h·ªá Ph√≤ng ƒê·∫£m b·∫£o ch·∫•t l∆∞·ª£ng v√† Kh·∫£o th√≠ qua email ldkham@bdu.edu.vn ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ chi ti·∫øt ·∫°. üéì"
        elif any(word in query_lower for word in ['k√™ khai', 'nhi·ªám v·ª•', 'gi·ªù chu·∫©n']):
            return "D·∫° th·∫ßy/c√¥, em ch∆∞a c√≥ th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y. Th·∫ßy/c√¥ c√≥ th·ªÉ li√™n h·ªá Ph√≤ng T·ªï ch·ª©c - C√°n b·ªô qua email tcccb@bdu.edu.vn ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ chi ti·∫øt ·∫°. üéì"
        elif any(word in query_lower for word in ['t·∫°p ch√≠', 'nghi√™n c·ª©u', 'khoa h·ªçc']):
            return "D·∫° th·∫ßy/c√¥, em ch∆∞a c√≥ th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y. Th·∫ßy/c√¥ c√≥ th·ªÉ li√™n h·ªá Ph√≤ng Nghi√™n c·ª©u - H·ª£p t√°c qua email nghiencuu@bdu.edu.vn ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ chi ti·∫øt ·∫°. üéì"
        else:
            return "D·∫° th·∫ßy/c√¥, em ch∆∞a c√≥ th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y. Th·∫ßy/c√¥ c√≥ th·ªÉ li√™n h·ªá ph√≤ng ban li√™n quan qua email info@bdu.edu.vn ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ chi ti·∫øt ·∫°. üéì"
    
    def _clean_query(self, query):
        """Clean and prepare query for lecturers"""
        if not query:
            return ""
        
        # Basic cleaning
        query = re.sub(r'\s+', ' ', query.strip())
        query = re.sub(r'[?]{2,}', '?', query)
        query = re.sub(r'[!]{2,}', '!', query)
        
        return query
    
    def _update_memory(self, session_id, query, intent_result, confidence, decision_type=None, was_education=True):
        """Enhanced memory update for lecturers with more context"""
        if session_id not in self.conversation_memory:
            self.conversation_memory[session_id] = []
        
        self.conversation_memory[session_id].append({
            'query': query,
            'intent': intent_result.get('intent', 'unknown'),
            'confidence': confidence,
            'timestamp': time.time(),
            'user_type': 'lecturer',  # Track that this is a lecturer session
            'decision_type': decision_type,  # ‚úÖ NEW: Track decision made
            'was_education_related': was_education,  # ‚úÖ NEW: Track if was education
            'is_education_query': self.decision_engine.is_education_related(query)  # ‚úÖ NEW: Direct check
        })
        
        # Keep last 10 interactions for lecturers (more history for work context)
        self.conversation_memory[session_id] = self.conversation_memory[session_id][-10:]
        
        logger.info(f"üß† Memory updated for session {session_id}: {len(self.conversation_memory[session_id])} total interactions")
    
    def _get_empty_query_response_lecturer(self):
        """Response for empty queries from lecturers"""
        return {
            'response': "D·∫° ch√†o th·∫ßy/c√¥! Em c√≥ th·ªÉ h·ªó tr·ª£ g√¨ cho th·∫ßy/c√¥ v·ªÅ c√¥ng vi·ªác t·∫°i BDU ·∫°? üéì",
            'confidence': 0.9,
            'method': 'empty_query_lecturer',
            'processing_time': 0.01
        }
    
    def get_conversation_context(self, session_id):
        """Get conversation context for a lecturer session"""
        return self.conversation_memory.get(session_id, [])
    
    def get_conversation_memory(self, session_id):
        """Get conversation memory from Gemini service"""
        return self.response_generator.get_conversation_memory(session_id)
    
    def clear_conversation_memory(self, session_id=None):
        """Clear conversation memory"""
        if session_id:
            self.response_generator.clear_conversation_memory(session_id)
            if session_id in self.conversation_memory:
                del self.conversation_memory[session_id]
        else:
            self.response_generator.clear_conversation_memory()
            self.conversation_memory.clear()


# Keep original ChatbotAI for retrieval (unchanged but enhanced for lecturers)
class ChatbotAI:
    def __init__(self):
        self.model = None
        self.index = None
        self.knowledge_data = []
        self.load_models()
    
    def load_models(self):
        """Load AI models and knowledge base"""
        try:
            self.model = SentenceTransformer('keepitreal/vietnamese-sbert')
            logger.info("‚úÖ Vietnamese SBERT loaded for lecturers")
            self.load_knowledge_base()
        except Exception as e:
            logger.error(f"Error loading models: {str(e)}")
            self.model = None
    
    def load_knowledge_base(self):
        """Load knowledge base from database and CSV with lecturer focus"""
        try:
            # Load from database
            db_knowledge = list(KnowledgeBase.objects.filter(is_active=True).values(
                'question', 'answer', 'category'
            ))
            
            # Load from CSV file - enhanced for lecturers
            csv_path = os.path.join(settings.BASE_DIR, 'data', 'QA.csv')
            csv_knowledge = []
            
            if os.path.exists(csv_path):
                df = pd.read_csv(csv_path, encoding='utf-8')
                if 'question' in df.columns and 'answer' in df.columns:
                    csv_knowledge = df[['question', 'answer']].fillna('').to_dict('records')
                    if 'category' in df.columns:
                        for i, item in enumerate(csv_knowledge):
                            item['category'] = df.iloc[i].get('category', 'Gi·∫£ng vi√™n')
                    else:
                        for item in csv_knowledge:
                            item['category'] = 'Gi·∫£ng vi√™n'
            
            # Combine sources with priority for lecturer-specific content
            self.knowledge_data = csv_knowledge + db_knowledge  # CSV first for lecturer priority
            
            # Build FAISS index
            if self.model and self.knowledge_data:
                self.build_faiss_index()
            
            logger.info(f"‚úÖ Loaded {len(self.knowledge_data)} knowledge entries for lecturers")
            
        except Exception as e:
            logger.error(f"Error loading knowledge: {str(e)}")
            self.knowledge_data = self.get_fallback_knowledge_lecturer()
    
    def get_fallback_knowledge_lecturer(self):
        """Fallback knowledge data specifically for lecturers"""
        return [
            {
                'question': 'ng√¢n h√†ng ƒë·ªÅ thi',
                'answer': 'Gi·∫£ng vi√™n c·∫ßn b√°o c√°o k·∫øt qu·∫£ x√¢y d·ª±ng ng√¢n h√†ng ƒë·ªÅ thi k·∫øt th√∫c h·ªçc ph·∫ßn v√† l·∫≠p k·∫ø ho·∫°ch cho h·ªçc k·ª≥ ti·∫øp theo. N·ªôp v·ªÅ Ph√≤ng ƒê·∫£m b·∫£o ch·∫•t l∆∞·ª£ng v√† Kh·∫£o th√≠ qua email ldkham@bdu.edu.vn tr∆∞·ªõc h·∫°n quy ƒë·ªãnh.',
                'category': 'Gi·∫£ng vi√™n'
            },
            {
                'question': 'k√™ khai nhi·ªám v·ª• nƒÉm h·ªçc',
                'answer': 'Gi·∫£ng vi√™n c∆° h·ªØu v√† th·ªânh gi·∫£ng c·∫ßn k√™ khai nhi·ªám v·ª• nƒÉm h·ªçc bao g·ªìm gi·∫£ng d·∫°y, nghi√™n c·ª©u khoa h·ªçc v√† c√°c ho·∫°t ƒë·ªông kh√°c. Khoa t·ªïng h·ª£p v√† b√°o c√°o l√™n nh√† tr∆∞·ªùng.',
                'category': 'Gi·∫£ng vi√™n'
            },
            {
                'question': 't·∫°p ch√≠ khoa h·ªçc',
                'answer': 'T·∫°p ch√≠ Khoa h·ªçc v√† C√¥ng ngh·ªá Tr∆∞·ªùng ƒê·∫°i h·ªçc B√¨nh D∆∞∆°ng nh·∫≠n b√†i vi·∫øt t·ª´ gi·∫£ng vi√™n, nghi√™n c·ª©u sinh v√† c√°c nh√† khoa h·ªçc. G·ª≠i b√†i qua email ch·ªâ ƒë·ªãnh c·ªßa t√≤a so·∫°n.',
                'category': 'Gi·∫£ng vi√™n'
            },
            {
                'question': 'thi ƒëua khen th∆∞·ªüng',
                'answer': 'Nh√† tr∆∞·ªùng t·ªï ch·ª©c ƒë√°nh gi√° thi ƒëua, khen th∆∞·ªüng c√° nh√¢n v√† t·∫≠p th·ªÉ xu·∫•t s·∫Øc trong nƒÉm h·ªçc. C√≥ c√°c danh hi·ªáu nh∆∞ Chi·∫øn sƒ© thi ƒëua, Lao ƒë·ªông ti√™n ti·∫øn...',
                'category': 'Gi·∫£ng vi√™n'
            }
        ]
    
    def build_faiss_index(self):
        """Build FAISS index for fast retrieval"""
        try:
            questions = [item['question'] for item in self.knowledge_data]
            embeddings = self.model.encode(questions)
            
            # Create FAISS index
            dimension = embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)
            
            # Normalize for cosine similarity
            faiss.normalize_L2(embeddings)
            self.index.add(embeddings.astype('float32'))
            
            logger.info(f"‚úÖ FAISS index built with {len(questions)} entries for lecturers")
            
        except Exception as e:
            logger.error(f"Error building FAISS index: {str(e)}")
            self.index = None
    
    def semantic_search(self, query, top_k=3):
        """Fast semantic search optimized for lecturer queries"""
        try:
            if not self.model or not self.index:
                return self.keyword_search(query)
            
            query_embedding = self.model.encode([query])
            faiss.normalize_L2(query_embedding)
            
            scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx < len(self.knowledge_data):
                    result = self.knowledge_data[idx].copy()
                    result['similarity'] = float(score)
                    results.append(result)
            
            return results[0] if results else (None, 0), results
            
        except Exception as e:
            logger.error(f"Semantic search error: {str(e)}")
            return self.keyword_search(query)
    
    def keyword_search(self, query):
        """Enhanced keyword fallback search for lecturers"""
        query_words = set(query.lower().split())
        best_match = None
        best_score = 0
        
        for item in self.knowledge_data:
            question_words = set(item['question'].lower().split())
            answer_words = set(item['answer'].lower().split())
            
            # Enhanced matching for lecturer-specific terms
            question_common = query_words & question_words
            answer_common = query_words & answer_words
            
            if question_common or answer_common:
                # Boost score for question matches
                question_score = len(question_common) / len(query_words | question_words) * 2
                answer_score = len(answer_common) / len(query_words | answer_words)
                
                total_score = question_score + answer_score
                
                if total_score > best_score:
                    best_score = total_score
                    best_match = item
        
        return best_match, best_score
    
    def generate_response(self, query):
        """Generate response optimized for lecturer hybrid system"""
        try:
            if not query.strip():
                return {
                    'response': 'D·∫° th·∫ßy/c√¥, vui l√≤ng nh·∫≠p c√¢u h·ªèi c·ª• th·ªÉ ·∫°. üéì',
                    'confidence': 0.1,
                    'method': 'empty_query',
                    'sources': []
                }
            
            # Search for match
            if self.model and self.index:
                best_match, all_results = self.semantic_search(query)
            else:
                best_match, confidence = self.keyword_search(query)
                all_results = [best_match] if best_match else []
            
            if best_match:
                similarity = best_match.get('similarity', confidence if 'confidence' in locals() else 0)
                
                return {
                    'response': best_match['answer'],
                    'confidence': similarity,
                    'method': 'retrieval',
                    'sources': self._format_sources(all_results[:2]),
                    'category': best_match.get('category', 'Gi·∫£ng vi√™n')
                }
            else:
                return {
                    'response': 'Em ch∆∞a c√≥ th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y.',
                    'confidence': 0.1,
                    'method': 'no_match',
                    'sources': []
                }
            
        except Exception as e:
            logger.error(f"Generate response error: {str(e)}")
            return {
                'response': 'ƒê√£ c√≥ l·ªói x·∫£y ra trong h·ªá th·ªëng.',
                'confidence': 0.1,
                'method': 'error',
                'sources': []
            }
    
    def _format_sources(self, results):
        """Format sources for display"""
        sources = []
        for result in results:
            if result and result.get('similarity', 0) > 0.2:
                sources.append({
                    'question': result['question'],
                    'category': result.get('category', 'Gi·∫£ng vi√™n'),
                    'similarity': result.get('similarity', 0)
                })
        return sources

# Create global instance optimized for lecturers
chatbot_ai = HybridChatbotAI()