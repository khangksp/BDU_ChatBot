import torch
import numpy as np
import logging
import re
from sklearn.metrics.pairwise import cosine_similarity
import time

# Try to import transformers, fallback if not available
try:
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("Warning: transformers not installed. PhoBERT will use fallback mode.")

logger = logging.getLogger(__name__)

class PhoBERTIntentClassifier:
    """Enhanced PhoBERT-based Intent Classification for BDU Lecturers - COMPLETE VERSION"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if TRANSFORMERS_AVAILABLE else None
        self.tokenizer = None
        self.model = None
        
        # ESSENTIAL: Set fallback_mode FIRST
        self.fallback_mode = True  # Default to fallback mode
        
        # Initialize components
        self.intent_categories = self._initialize_lecturer_intents()
        self.entity_patterns = self._initialize_lecturer_entities()
        
        # ‚úÖ TH√äM: Initialize normalizer BEFORE model loading
        self.normalizer = None
        try:
            from .vietnamese_normalizer import VietnameseNormalizer
            self.normalizer = VietnameseNormalizer()
            print("‚úÖ Vietnamese Normalizer initialized successfully")
        except ImportError as e:
            print(f"‚ùå Failed to import VietnameseNormalizer: {e}")
            # Create dummy normalizer
            self.normalizer = self._create_dummy_normalizer()
        
        # Try to load model only if transformers available
        if TRANSFORMERS_AVAILABLE:
            try:
                self.load_model()
            except Exception as e:
                logger.warning(f"PhoBERT model failed to load: {str(e)}")
                self.fallback_mode = True
        else:
            logger.warning("PhoBERT running in enhanced fallback mode (keyword-based) for lecturers")

    def _create_dummy_normalizer(self):
        """Create dummy normalizer if import fails"""
        class DummyNormalizer:
            def normalize_query(self, query):
                return query.lower().strip()
            
            def create_search_variants(self, query):
                return [query, query.lower()]
        
        return DummyNormalizer()
    
    def _initialize_lecturer_intents(self):
        """Comprehensive intent categories specifically for BDU lecturers"""
        return {
            'greeting': {
                'keywords': ['xin ch√†o', 'hello', 'hi', 'ch√†o th·∫ßy', 'ch√†o c√¥', 'halo', 'ch√†o', 'hey'],
                'confidence_threshold': 0.6,
                'description': 'Ch√†o h·ªèi',
                'response_style': 'friendly'
            },
            
            # ‚úÖ LECTURER-SPECIFIC INTENTS based on QA.csv analysis
            'bank_exam_questions': {
                'keywords': ['ng√¢n h√†ng ƒë·ªÅ thi', 'ngan hang de thi', 'ƒë·ªÅ thi', 'de thi', 'b√°o c√°o ƒë·ªÅ thi', 'bao cao de thi', 'k·∫ø ho·∫°ch ƒë·ªÅ thi', 'ke hoach de thi', 'file m·ªÅm', 'file mem'],
                'confidence_threshold': 0.4,
                'description': 'Ng√¢n h√†ng ƒë·ªÅ thi',
                'response_style': 'detailed'
            },
            
            'annual_task_declaration': {
                'keywords': ['k√™ khai nhi·ªám v·ª•', 'ke khai nhiem vu', 'nhi·ªám v·ª• nƒÉm h·ªçc', 'nhiem vu nam hoc', 'k√™ khai', 'ke khai', 'gi·ªù chu·∫©n', 'gio chuan', 'gi·∫£ng vi√™n c∆° h·ªØu', 'giang vien co huu', 'th·ªânh gi·∫£ng', 'thinh giang'],
                'confidence_threshold': 0.4,
                'description': 'K√™ khai nhi·ªám v·ª• nƒÉm h·ªçc',
                'response_style': 'informative'
            },
            
            'academic_journal': {
                'keywords': ['t·∫°p ch√≠', 'tap chi', 't·∫°p ch√≠ khoa h·ªçc', 'tap chi khoa hoc', 'b√†i vi·∫øt', 'bai viet', 'nghi√™n c·ª©u', 'nghien cuu', 'khoa h·ªçc c√¥ng ngh·ªá', 'khoa hoc cong nghe', 'g·ª≠i b√†i', 'gui bai'],
                'confidence_threshold': 0.4,
                'description': 'T·∫°p ch√≠ khoa h·ªçc',
                'response_style': 'detailed'
            },
            
            'competition_awards': {
                'keywords': ['thi ƒëua', 'thi dua', 'khen th∆∞·ªüng', 'khen thuong', 'danh hi·ªáu', 'danh hieu', 'b·∫±ng khen', 'bang khen', 'l·ªÖ khen th∆∞·ªüng', 'le khen thuong', 'chi·∫øn sƒ© thi ƒëua', 'chien si thi dua', 'lao ƒë·ªông ti√™n ti·∫øn', 'lao dong tien tien'],
                'confidence_threshold': 0.4,
                'description': 'Thi ƒëua khen th∆∞·ªüng',
                'response_style': 'encouraging'
            },
            
            'reports_deadlines': {
                'keywords': ['b√°o c√°o', 'bao cao', 'n·ªôp', 'nop', 'h·∫°n cu·ªëi', 'han cuoi', 'deadline', 'th·ªùi h·∫°n', 'thoi han', 'g·ª≠i v·ªÅ', 'gui ve', 'ph√≤ng ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng', 'phong dam bao chat luong'],
                'confidence_threshold': 0.4,
                'description': 'B√°o c√°o v√† th·ªß t·ª•c',
                'response_style': 'urgent'
            },
            
            'teaching_schedule': {
                'keywords': ['l·ªãch gi·∫£ng d·∫°y', 'lich giang day', 'th·ªùi kh√≥a bi·ªÉu', 'thoi khoa bieu', 'l·ªãch h·ªçc', 'lich hoc', 'c·∫≠p nh·∫≠t d·ªØ li·ªáu', 'cap nhat du lieu', 'ph·∫ßn m·ªÅm qu·∫£n l√Ω', 'phan mem quan ly', 'ƒë√†o t·∫°o', 'dao tao'],
                'confidence_threshold': 0.4,
                'description': 'L·ªãch gi·∫£ng d·∫°y',
                'response_style': 'informative'
            },
            
            'quality_assurance': {
                'keywords': ['ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng', 'dam bao chat luong', 'ki·ªÉm tra', 'kiem tra', 'gi√°m s√°t', 'giam sat', 'ƒë√°nh gi√°', 'danh gia', 'chu·∫©n ƒë·∫ßu ra', 'chuan dau ra', 'ti√™u chu·∫©n', 'tieu chuan'],
                'confidence_threshold': 0.5,
                'description': 'ƒê·∫£m b·∫£o ch·∫•t l∆∞·ª£ng',
                'response_style': 'detailed'
            },
            
            'departments_contacts': {
                'keywords': ['ph√≤ng ban', 'phong ban', 'li√™n h·ªá', 'lien he', 'email', 'phone', 'contact', 'ph√≤ng t·ªï ch·ª©c', 'phong to chuc', 'ph√≤ng nghi√™n c·ª©u', 'phong nghien cuu', 'ph√≤ng kh·∫£o th√≠', 'phong khao thi'],
                'confidence_threshold': 0.4,
                'description': 'Th√¥ng tin ph√≤ng ban',
                'response_style': 'helpful'
            },
            
            # ‚úÖ GENERAL EDUCATION INTENTS (kept from original)
            'admission_general': {
                'keywords': ['tuy·ªÉn sinh', 'tuyen sinh', 'nh·∫≠p h·ªçc', 'ƒëƒÉng k√Ω h·ªçc', 'v√†o tr∆∞·ªùng', 'ƒëi·ªÅu ki·ªán tuy·ªÉn sinh', 'x√©t tuy·ªÉn'],
                'confidence_threshold': 0.5,
                'description': 'Th√¥ng tin tuy·ªÉn sinh chung',
                'response_style': 'informative'
            },
            'tuition_general': {
                'keywords': ['h·ªçc ph√≠', 'hoc phi', 'hp', 'chi ph√≠', 'chi phi', 'ti·ªÅn h·ªçc', 'tien hoc', 'm·ª©c ph√≠', 'muc phi', 'ph√≠ h·ªçc t·∫≠p', 'phi hoc tap'],
                'confidence_threshold': 0.4,
                'description': 'H·ªçc ph√≠ chung',
                'response_style': 'informative'
            },
            'programs': {
                'keywords': ['ng√†nh', 'nganh', 'chuy√™n ng√†nh', 'chuyen nganh', 'ƒë√†o t·∫°o', 'dao tao', 'ch∆∞∆°ng tr√¨nh h·ªçc', 'chuong trinh hoc'],
                'confidence_threshold': 0.5,
                'description': 'Ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o',
                'response_style': 'informative'
            },
            'facilities': {
                'keywords': ['c∆° s·ªü v·∫≠t ch·∫•t', 'co so vat chat', 'ph√≤ng h·ªçc', 'phong hoc', 'th∆∞ vi·ªán', 'thu vien', 'lab', 'k√Ω t√∫c x√°', 'ky tuc xa', 'ti·ªán √≠ch', 'tien ich'],
                'confidence_threshold': 0.6,
                'description': 'C∆° s·ªü v·∫≠t ch·∫•t',
                'response_style': 'descriptive'
            },
            
            # ‚úÖ CLARIFICATION AND VAGUE INTENTS
            'clarification_needed': {
                'keywords': ['g√¨', 'gi', 'sao', 'n√†o', 'nao', 'nh∆∞ th·∫ø n√†o', 'nhu the nao', 'l√†m sao', 'lam sao', 'c√°ch n√†o', 'cach nao', 'th·∫ø n√†o', 'the nao'],
                'confidence_threshold': 0.2,
                'description': 'C·∫ßn l√†m r√µ',
                'response_style': 'clarifying'
            },
            
            'general': {
                'keywords': ['th√¥ng tin', 'thong tin', 'h·ªó tr·ª£', 'ho tro', 'gi√∫p', 'giup', 'h∆∞·ªõng d·∫´n', 'huong dan', 'bdu', 'ƒë·∫°i h·ªçc b√¨nh d∆∞∆°ng', 'dai hoc binh duong'],
                'confidence_threshold': 0.2,
                'description': 'C√¢u h·ªèi chung',
                'response_style': 'neutral'
            }
        }
    
    def _initialize_lecturer_entities(self):
        """Enhanced entity patterns for lecturers"""
        return {
            'lecturer_departments': [
                'ph√≤ng ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng', 'ph√≤ng kh·∫£o th√≠', 'ph√≤ng t·ªï ch·ª©c c√°n b·ªô',
                'ph√≤ng nghi√™n c·ª©u h·ª£p t√°c', 'ph√≤ng ƒë√†o t·∫°o', 'ph√≤ng c√¥ng t√°c sinh vi√™n',
                'phong dam bao chat luong', 'phong khao thi', 'phong to chuc can bo',
                'phong nghien cuu hop tac', 'phong dao tao', 'phong cong tac sinh vien'
            ],
            'lecturer_positions': [
                'gi·∫£ng vi√™n', 'gi·∫£ng vi√™n c∆° h·ªØu', 'gi·∫£ng vi√™n th·ªânh gi·∫£ng',
                'ph√≥ gi√°o s∆∞', 'ti·∫øn sƒ©', 'th·∫°c sƒ©', 'tr∆∞·ªüng khoa', 'ph√≥ khoa',
                'giang vien', 'giang vien co huu', 'giang vien thinh giang',
                'pho giao su', 'tien si', 'thac si', 'truong khoa', 'pho khoa'
            ],
            'document_types': [
                'b√°o c√°o', 'k·∫ø ho·∫°ch', 'th√¥ng b√°o', 'quy·∫øt ƒë·ªãnh', 'file m·ªÅm',
                'bao cao', 'ke hoach', 'thong bao', 'quyet dinh', 'file mem',
                'vƒÉn b·∫£n', 'h·ªì s∆°', 't√†i li·ªáu', 'van ban', 'ho so', 'tai lieu'
            ],
            'time_expressions': [
                'nƒÉm h·ªçc 2023-2024', 'h·ªçc k·ª≥ I', 'h·ªçc k·ª≥ II', 'k·ª≥ h√®',
                'nam hoc 2023-2024', 'hoc ky I', 'hoc ky II', 'ky he',
                'tr∆∞·ªõc ng√†y', 'h·∫°n cu·ªëi', 'deadline', 'truoc ngay', 'han cuoi'
            ],
            'lecturer_activities': [
                'gi·∫£ng d·∫°y', 'nghi√™n c·ª©u khoa h·ªçc', 'ph·ª•c v·ª• c·ªông ƒë·ªìng',
                'giang day', 'nghien cuu khoa hoc', 'phuc vu cong dong',
                'thi ƒëua', 'khen th∆∞·ªüng', 'ƒë√°nh gi√°', 'thi dua', 'khen thuong', 'danh gia'
            ],
            'majors': [
                'c√¥ng ngh·ªá th√¥ng tin', 'cntt', 'it', 'khoa h·ªçc m√°y t√≠nh', 'tin h·ªçc',
                'kinh t·∫ø', 'qu·∫£n tr·ªã kinh doanh', 'marketing', 't√†i ch√≠nh ng√¢n h√†ng', 'k·∫ø to√°n',
                'lu·∫≠t', 'lu·∫≠t h·ªçc', 'ph√°p l√Ω', 'lu·∫≠t kinh t·∫ø',
                'y khoa', 'y h·ªçc', 'b√°c sƒ©', 'ƒëi·ªÅu d∆∞·ª°ng', 'd∆∞·ª£c', 'y t·∫ø',
                'k·ªπ thu·∫≠t', 'x√¢y d·ª±ng', 'c∆° kh√≠', 'ƒëi·ªán', 'ƒëi·ªán t·ª≠', 'oto'
            ],
            'emotions': [
                'c·∫ßn g·∫•p', 'kh·∫©n c·∫•p', 'urgent', 'quan tr·ªçng', '∆∞u ti√™n',
                'can gap', 'khan cap', 'quan trong', 'uu tien',
                'lo l·∫Øng', 'kh√≥ khƒÉn', 'cƒÉng th·∫≥ng', 'stress',
                'lo lang', 'kho khan', 'cang thang'
            ]
        }
    
    def load_model(self):
        """Load PhoBERT model with enhanced error handling"""
        try:
            if not TRANSFORMERS_AVAILABLE:
                raise ImportError("Transformers not available")
                
            model_name = "vinai/phobert-base"
            logger.info(f"Loading PhoBERT model: {model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModel.from_pretrained(model_name)
            self.model.to(self.device)
            self.model.eval()
            
            # Only set fallback_mode to False if everything loaded successfully
            self.fallback_mode = False
            logger.info("‚úÖ PhoBERT model loaded successfully for lecturers")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è PhoBERT not available, using enhanced fallback for lecturers: {str(e)}")
            self.tokenizer = None
            self.model = None
            self.fallback_mode = True  # Ensure fallback mode is set
    
    def classify_intent(self, query):
        """Enhanced intent classification with normalization for lecturers"""
        if not query or not query.strip():
            return {
                'intent': 'general',
                'confidence': 0.3,
                'description': 'C√¢u h·ªèi chung',
                'response_style': 'neutral'
            }
        
        # ‚úÖ CRITICAL: Check if normalizer exists
        if not self.normalizer:
            print("‚ùå NORMALIZER ERROR: Normalizer not available")
            query_variants = [query, query.lower()]
            normalized_query = query.lower()
        else:
            # NORMALIZE QUERY FIRST
            normalized_query = self.normalizer.normalize_query(query)
            query_variants = self.normalizer.create_search_variants(query)
        
        print(f"üîç LECTURER INTENT DEBUG: Original = '{query}'")
        print(f"üîç LECTURER INTENT DEBUG: Normalized = '{normalized_query}'")
        print(f"üîç LECTURER INTENT DEBUG: Variants = {query_variants}")
        
        intent_scores = {}
        
        # Method 1: Enhanced keyword matching with variants for lecturers
        for variant in query_variants:
            variant_lower = variant.lower().strip()
            
            for intent, config in self.intent_categories.items():
                score = 0
                keyword_matches = 0
                
                for keyword in config['keywords']:
                    if keyword in variant_lower:
                        # ‚úÖ ENHANCED: Boost score for lecturer-specific terms
                        if intent.startswith(('bank_exam', 'annual_task', 'academic_journal', 'competition_awards')):
                            score += 2.5  # Higher weight for lecturer-specific intents
                        elif keyword == variant_lower:
                            score += 2
                        elif variant_lower.startswith(keyword) or variant_lower.endswith(keyword):
                            score += 1.5
                        else:
                            score += 1
                        keyword_matches += 1
                
                # Normalize and boost for multiple keyword matches
                if len(config['keywords']) > 0:
                    base_score = score / len(config['keywords'])
                    # Bonus for multiple keyword matches
                    if keyword_matches > 1:
                        base_score *= (1 + (keyword_matches - 1) * 0.3)  # Increased bonus
                    
                    # Update intent score with max from all variants
                    intent_scores[intent] = max(intent_scores.get(intent, 0), min(base_score, 1.0))
        
        print(f"üîç LECTURER INTENT DEBUG: Intent scores = {intent_scores}")
        
        # Method 2: Context-based boosting with normalized query for lecturers
        self._boost_lecturer_contextual_intents(normalized_query.lower(), intent_scores)
        
        print(f"üîç LECTURER INTENT DEBUG: After lecturer boosting = {intent_scores}")
        
        # Method 3: PhoBERT similarity (if available)
        if not self.fallback_mode and self.model and self.tokenizer:
            try:
                # Use normalized query for semantic similarity
                self._add_semantic_similarity(normalized_query, intent_scores)
            except Exception as e:
                logger.warning(f"Semantic similarity failed, using fallback: {str(e)}")
        
        # Find best intent
        if intent_scores:
            best_intent = max(intent_scores.items(), key=lambda x: x[1])
            intent_name, confidence = best_intent
            
            print(f"üîç LECTURER INTENT DEBUG: Best intent = {intent_name}, confidence = {confidence}")
            
            # ‚úÖ Dynamic threshold based on query complexity for lecturers
            base_threshold = self.intent_categories[intent_name]['confidence_threshold']
            if self.fallback_mode:
                threshold = base_threshold * 0.3  # ‚úÖ VERY LOW for lecturer fallback
            else:
                threshold = base_threshold * 0.5  # ‚úÖ LOWER with normalization
            
            print(f"üîç LECTURER INTENT DEBUG: Threshold = {threshold}, base = {base_threshold}")
            
            if confidence >= threshold:
                print(f"üîç LECTURER INTENT DEBUG: INTENT MATCHED!")
                return {
                    'intent': intent_name,
                    'confidence': confidence,
                    'description': self.intent_categories[intent_name]['description'],
                    'response_style': self.intent_categories[intent_name]['response_style'],
                    'normalized_query': normalized_query,
                    'lecturer_optimized': True
                }
        
        print(f"üîç LECTURER INTENT DEBUG: NO INTENT MATCHED - using general")
        return {
            'intent': 'general',
            'confidence': 0.3,
            'description': 'C√¢u h·ªèi chung',
            'response_style': 'neutral',
            'normalized_query': normalized_query,
            'lecturer_optimized': True
        }
    
    def _boost_lecturer_contextual_intents(self, query_lower, intent_scores):
        """Boost intent scores based on lecturer-specific context"""
        
        # ‚úÖ LECTURER-SPECIFIC: Department context
        if any(phrase in query_lower for phrase in ['ph√≤ng ƒë·∫£m b·∫£o', 'ph√≤ng kh·∫£o th√≠', 'phong dam bao', 'phong khao thi']):
            intent_scores['bank_exam_questions'] = intent_scores.get('bank_exam_questions', 0) + 0.4
            intent_scores['quality_assurance'] = intent_scores.get('quality_assurance', 0) + 0.3
        
        if any(phrase in query_lower for phrase in ['ph√≤ng t·ªï ch·ª©c', 'ph√≤ng c√°n b·ªô', 'phong to chuc', 'phong can bo']):
            intent_scores['annual_task_declaration'] = intent_scores.get('annual_task_declaration', 0) + 0.4
            intent_scores['competition_awards'] = intent_scores.get('competition_awards', 0) + 0.3
        
        # ‚úÖ LECTURER-SPECIFIC: Urgency context
        if any(word in query_lower for word in ['h·∫°n cu·ªëi', 'deadline', 'g·∫•p', 'kh·∫©n c·∫•p', 'han cuoi', 'gap', 'khan cap']):
            intent_scores['reports_deadlines'] = intent_scores.get('reports_deadlines', 0) + 0.5
        
        # ‚úÖ LECTURER-SPECIFIC: Academic context
        if any(word in query_lower for word in ['nghi√™n c·ª©u', 'b√†i vi·∫øt', 't·∫°p ch√≠', 'nghien cuu', 'bai viet', 'tap chi']):
            intent_scores['academic_journal'] = intent_scores.get('academic_journal', 0) + 0.4
        
        # ‚úÖ LECTURER-SPECIFIC: Teaching context
        if any(word in query_lower for word in ['gi·∫£ng d·∫°y', 'l·ªãch h·ªçc', 'th·ªùi kh√≥a bi·ªÉu', 'giang day', 'lich hoc', 'thoi khoa bieu']):
            intent_scores['teaching_schedule'] = intent_scores.get('teaching_schedule', 0) + 0.4
        
        # ‚úÖ LECTURER-SPECIFIC: Awards context
        if any(word in query_lower for word in ['thi ƒëua', 'khen th∆∞·ªüng', 'danh hi·ªáu', 'thi dua', 'khen thuong', 'danh hieu']):
            intent_scores['competition_awards'] = intent_scores.get('competition_awards', 0) + 0.4
        
        # Question patterns (enhanced for lecturers)
        if query_lower.endswith('?'):
            # Questions from lecturers tend to be more specific
            for intent in ['bank_exam_questions', 'annual_task_declaration', 'academic_journal', 'reports_deadlines']:
                if intent in intent_scores:
                    intent_scores[intent] += 0.2
        
        # Vague questions that need clarification
        vague_indicators = ['g√¨', 'sao', 'n√†o', 'nh∆∞ th·∫ø n√†o', 'gi', 'nao', 'nhu the nao']
        if any(word in query_lower for word in vague_indicators) and len(query_lower.split()) <= 5:
            intent_scores['clarification_needed'] = intent_scores.get('clarification_needed', 0) + 0.3
    
    def _add_semantic_similarity(self, query, intent_scores):
        """Add PhoBERT semantic similarity scores"""
        try:
            if self.fallback_mode or not self.model or not self.tokenizer:
                return
                
            query_embedding = self.encode_text(query)
            if query_embedding is not None:
                for intent, config in self.intent_categories.items():
                    # Create comprehensive intent representation
                    intent_text = f"{config['description']} {' '.join(config['keywords'][:5])}"
                    intent_embedding = self.encode_text(intent_text)
                    
                    if intent_embedding is not None:
                        similarity = cosine_similarity(query_embedding, intent_embedding)[0][0]
                        # Blend with keyword score
                        current_score = intent_scores.get(intent, 0)
                        blended_score = (current_score * 0.7) + (similarity * 0.3)  # Favor keywords more
                        intent_scores[intent] = blended_score
                        
        except Exception as e:
            logger.warning(f"Semantic similarity failed: {str(e)}")
    
    def encode_text(self, text):
        """Encode text using PhoBERT with error handling"""
        if self.fallback_mode or not self.model or not self.tokenizer:
            return None
        
        try:
            inputs = self.tokenizer(text, return_tensors="pt", 
                                  padding=True, truncation=True, max_length=256)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                embeddings = outputs.pooler_output
            
            return embeddings.cpu().numpy()
        except Exception as e:
            logger.error(f"Error encoding text: {str(e)}")
            return None
    
    def extract_entities(self, query):
        """Enhanced entity extraction for lecturers"""
        if not query:
            return {}
            
        query_lower = query.lower()
        entities = {}
        
        # ‚úÖ LECTURER-SPECIFIC: Extract departments with confidence
        for dept in self.entity_patterns['lecturer_departments']:
            if dept in query_lower:
                entities['department'] = dept
                entities['department_confidence'] = 1.0 if dept == query_lower else 0.9
                break
        
        # ‚úÖ LECTURER-SPECIFIC: Extract positions
        for position in self.entity_patterns['lecturer_positions']:
            if position in query_lower:
                entities['position'] = position
                entities['position_confidence'] = 1.0 if position == query_lower else 0.8
                break
        
        # ‚úÖ LECTURER-SPECIFIC: Extract document types
        for doc_type in self.entity_patterns['document_types']:
            if doc_type in query_lower:
                entities['document_type'] = doc_type
                break
        
        # ‚úÖ LECTURER-SPECIFIC: Extract lecturer activities
        for activity in self.entity_patterns['lecturer_activities']:
            if activity in query_lower:
                entities['activity'] = activity
                break
        
        # Extract majors with confidence
        for major in self.entity_patterns['majors']:
            if major in query_lower:
                entities['major'] = major
                entities['major_confidence'] = 1.0 if major == query_lower else 0.8
                break
        
        # Extract time expressions
        for time_expr in self.entity_patterns['time_expressions']:
            if time_expr in query_lower:
                entities['time'] = time_expr
                break
        
        # ‚úÖ ENHANCED: Extract emotions with lecturer-specific intensity
        emotion_intensity = 0
        detected_emotion = None
        for emotion in self.entity_patterns['emotions']:
            if emotion in query_lower:
                # Lecturer-specific emotions get different intensity
                if emotion in ['c·∫ßn g·∫•p', 'kh·∫©n c·∫•p', 'urgent', 'can gap', 'khan cap']:
                    emotion_intensity = 0.9  # High urgency for lecturers
                elif emotion in ['quan tr·ªçng', '∆∞u ti√™n', 'quan trong', 'uu tien']:
                    emotion_intensity = 0.8
                elif emotion in ['lo l·∫Øng', 'kh√≥ khƒÉn', 'lo lang', 'kho khan']:
                    emotion_intensity = 0.7
                else:
                    emotion_intensity = 0.6
                detected_emotion = emotion
                break
        
        if detected_emotion:
            entities['emotion'] = detected_emotion
            entities['emotion_intensity'] = emotion_intensity
        
        return entities
    
    def analyze_query(self, query):
        """Comprehensive query analysis with safe fallbacks for lecturers"""
        start_time = time.time()
        
        try:
            intent_result = self.classify_intent(query)
            entities = self.extract_entities(query)
            
            # ‚úÖ ENHANCED: Additional analysis for lecturers
            analysis = {
                'intent': intent_result,
                'entities': entities,
                'query_length': len(query) if query else 0,
                'word_count': len(query.split()) if query else 0,
                'is_question': '?' in query if query else False,
                'urgency': self._detect_lecturer_urgency(query),
                'complexity': self._assess_lecturer_complexity(query),
                'sentiment': self._detect_lecturer_sentiment(query, entities),
                'processing_time': time.time() - start_time,
                'fallback_mode': self.fallback_mode,
                'lecturer_optimized': True
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"Query analysis error: {str(e)}")
            # Safe fallback for lecturers
            return {
                'intent': {
                    'intent': 'general',
                    'confidence': 0.3,
                    'description': 'C√¢u h·ªèi chung',
                    'response_style': 'neutral'
                },
                'entities': {},
                'query_length': len(query) if query else 0,
                'word_count': len(query.split()) if query else 0,
                'is_question': False,
                'urgency': 'normal',
                'complexity': 'simple',
                'sentiment': 'neutral',
                'processing_time': time.time() - start_time,
                'fallback_mode': True,
                'lecturer_optimized': True
            }
    
    def _detect_lecturer_urgency(self, query):
        """Detect urgency level specifically for lecturers"""
        if not query:
            return 'normal'
            
        # ‚úÖ LECTURER-SPECIFIC urgent terms
        urgent_words = ['g·∫•p', 'urgent', 'kh·∫©n c·∫•p', 'c·∫ßn ngay', 'h·∫°n cu·ªëi', 'deadline', 
                       'gap', 'khan cap', 'can ngay', 'han cuoi']
        medium_urgent_words = ['s·ªõm', 'nhanh ch√≥ng', '∆∞u ti√™n', 'quan tr·ªçng',
                              'som', 'nhanh chong', 'uu tien', 'quan trong']
        
        query_lower = query.lower()
        
        if any(word in query_lower for word in urgent_words):
            return 'high'
        elif any(word in query_lower for word in medium_urgent_words):
            return 'medium'
        else:
            return 'normal'
    
    def _assess_lecturer_complexity(self, query):
        """Assess query complexity for lecturers"""
        if not query:
            return 'simple'
            
        word_count = len(query.split())
        question_marks = query.count('?')
        
        # ‚úÖ LECTURER-SPECIFIC: Consider technical terms
        technical_terms = ['ng√¢n h√†ng ƒë·ªÅ thi', 'k√™ khai nhi·ªám v·ª•', 't·∫°p ch√≠ khoa h·ªçc', 
                          'ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng', 'ph·∫ßn m·ªÅm qu·∫£n l√Ω']
        has_technical = any(term in query.lower() for term in technical_terms)
        
        if (word_count > 20 or question_marks > 1) or has_technical:
            return 'complex'
        elif word_count > 10:
            return 'medium'
        else:
            return 'simple'
    
    def _detect_lecturer_sentiment(self, query, entities):
        """Detect overall sentiment for lecturers"""
        if not query:
            return 'neutral'
            
        # ‚úÖ LECTURER-SPECIFIC sentiment words
        positive_words = ['t·ªët', 'hay', 'th√≠ch', 'mu·ªën', 'quan t√¢m', 'h√†o h·ª©ng', 'h·ªó tr·ª£',
                         'tot', 'thich', 'quan tam', 'hao hung', 'ho tro']
        negative_words = ['kh√≥ khƒÉn', 'lo l·∫Øng', 'kh√¥ng', 'ch√°n', 't·ªá', 'v·∫•n ƒë·ªÅ', 'l·ªói',
                         'kho khan', 'lo lang', 'khong', 'chan', 'te', 'van de', 'loi']
        urgent_words = ['g·∫•p', 'kh·∫©n c·∫•p', 'c·∫ßn ngay', 'gap', 'khan cap', 'can ngay']
        
        query_lower = query.lower()
        positive_count = sum(1 for word in positive_words if word in query_lower)
        negative_count = sum(1 for word in negative_words if word in query_lower)
        urgent_count = sum(1 for word in urgent_words if word in query_lower)
        
        # Factor in emotional entities
        if entities and 'emotion' in entities:
            emotion = entities['emotion']
            if emotion in ['quan tr·ªçng', '∆∞u ti√™n', 'quan trong', 'uu tien']:
                positive_count += 1
            elif emotion in ['lo l·∫Øng', 'kh√≥ khƒÉn', 'lo lang', 'kho khan']:
                negative_count += 2
            elif emotion in ['c·∫ßn g·∫•p', 'kh·∫©n c·∫•p', 'can gap', 'khan cap']:
                urgent_count += 2
        
        # ‚úÖ LECTURER-SPECIFIC: Urgency is often neutral/professional
        if urgent_count > 0:
            return 'urgent'
        elif positive_count > negative_count:
            return 'positive'
        elif negative_count > positive_count:
            return 'negative'
        else:
            return 'neutral'

    # COMPATIBILITY METHODS - Enhanced for lecturers
    def get_system_status(self):
        """Get PhoBERT system status for lecturers"""
        return {
            'model_loaded': bool(self.model),
            'fallback_mode': self.fallback_mode,
            'transformers_available': TRANSFORMERS_AVAILABLE,
            'device': str(self.device) if self.device else 'cpu',
            'intents_available': len(self.intent_categories),
            'lecturer_intents': [
                'bank_exam_questions', 'annual_task_declaration', 'academic_journal',
                'competition_awards', 'reports_deadlines', 'teaching_schedule',
                'quality_assurance', 'departments_contacts'
            ],
            'lecturer_optimized': True,
            'features': [
                'lecturer_specific_intents',
                'department_entity_extraction',
                'urgency_detection',
                'clarification_detection',
                'vietnamese_normalization'
            ]
        }